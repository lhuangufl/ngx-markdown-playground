import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.functions; 
import scala.collection.JavaConverters; 

import java.util.Arrays;
import java.util.List;
import java.util.Map;

public class SbmFactorPairingProcessor {

    public Dataset<Row> process(SparkSession spark, Map<String, Dataset<Row>> dsMap) {
        Dataset<Row> inputDs = dsMap.get("SBM_DELTA_VEGA_SENSITIVITIES");
        if (inputDs == null) throw new RuntimeException("Input missing: SBM_DELTA_VEGA_SENSITIVITIES");

        // 1. Projection & Deduplication
        // Explicitly select columns to minimize memory usage and ensure index stability.
        String[] colsOfInterest = {
            "REGIME", "RISK_CLASS", "RISK_BUCKET", "RISK_FACTOR_SCENARIO_FAMILY",
            "RISK_FACTOR_TENOR", "RISK_FACTOR_TYPE", "ISSUER_ID"
        };
        
        // Apply .na().fill("") to mimic the original 'sStr()' logic. 
        // This ensures nulls are treated as empty strings for safe joining.
        Dataset<Row> distinctFactorsDs = inputDs.selectExpr(colsOfInterest)
            .na().fill("") 
            .distinct();

        // 2. Self-Join Preparation
        // Alias the DataFrame to treat it as two separate logical tables (Left and Right).
        Dataset<Row> left = distinctFactorsDs.alias("L");
        Dataset<Row> right = distinctFactorsDs.alias("R");

        // 3. Define Join Conditions
        // These columns correspond to the keys used in the original 'getJoinKey' method.
        List<String> joinKeys = Arrays.asList(
            "REGIME", "RISK_CLASS", "RISK_BUCKET", "RISK_FACTOR_SCENARIO_FAMILY"
        );
        
        // Convert Java List to Scala Seq required by the Spark API.
        scala.collection.Seq<String> joinCols = JavaConverters.asScalaBufferConverter(joinKeys).asScala().toSeq();

        // 4. Execute Distributed Join
        // This leverages the cluster's power (HashJoin/Shuffle) instead of looping on the Driver.
        Dataset<Row> joinedDs = left.join(right, joinCols, "inner");

        // 5. Column Selection & Renaming
        // Reconstruct the output schema to match the original requirements.
        Dataset<Row> result = joinedDs.select(
            // Shared Keys (Selecting from Left side)
            functions.col("L.REGIME"),
            functions.col("L.RISK_CLASS"),
            functions.col("L.RISK_BUCKET"),
            functions.col("L.RISK_FACTOR_SCENARIO_FAMILY"),
            
            // Left Side Attributes (T.*)
            functions.col("L.RISK_FACTOR_TENOR"),
            functions.col("L.RISK_FACTOR_TYPE"),
            functions.col("L.ISSUER_ID"),
            
            // Right Side Attributes (T1.* -> _1 suffix)
            // Note: RISK_BUCKET is strictly redundant based on the join key, but included to match schema.
            functions.col("R.RISK_BUCKET").as("RISK_BUCKET_1"), 
            functions.col("R.RISK_FACTOR_TENOR").as("RISK_FACTOR_TENOR_1"),
            functions.col("R.RISK_FACTOR_TYPE").as("RISK_FACTOR_TYPE_1"),
            functions.col("R.ISSUER_ID").as("ISSUER_ID_1")
        );

        return result;
    }
}
