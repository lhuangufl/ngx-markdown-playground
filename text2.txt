import org.apache.spark.api.java.JavaSparkContext; // 必须引入
import org.apache.spark.api.java.function.FlatMapGroupsFunction; // 修正为 FlatMap
import org.apache.spark.api.java.function.MapFunction;
import org.apache.spark.broadcast.Broadcast;
import org.apache.spark.sql.*;
import org.apache.spark.sql.catalyst.encoders.RowEncoder;
import org.apache.spark.sql.types.*;
import java.util.*;
import java.util.stream.*;

/**
 * Main execution method containing all business logic.
 */
public Dataset<Row> process(SparkSession spark, Map<String, Dataset<Row>> dsMap) {

    // =========================================================================
    // 0. Global Setup & Broadcast
    // =========================================================================
    Dataset<Row> inputDs = dsMap.get("SBM_DELTA_VEGA_SENSITIVITIES");
    Dataset<Row> correlationDs = dsMap.get("DELTA_VEGA_INTRA_BUCKET_RESULT_CORRELATION");

    if (inputDs == null || correlationDs == null) {
        throw new RuntimeException("Missing required inputs.");
    }

    // Tool: Local helper for null-safe operations
    class Tool {
        String s(Object o) { return (o == null) ? "" : o.toString().trim(); }
        double d(Object o) { return (o instanceof Number) ? ((Number) o).doubleValue() : 0.0; }
        String extractIssuer(String name) {
            if (name == null || !name.contains("_")) return name == null ? "" : name;
            return name.substring(name.lastIndexOf("_") + 1);
        }
    }
    final Tool tool = new Tool();

    // 1. Prepare Correlation Map (Driver Side)
    Map<String, List<Row>> localCorrMap = new HashMap<>();
    List<Row> corrList = correlationDs.collectAsList();
    for (Row r : corrList) {
        String key = tool.s(r.getAs("REGIME")) + "|" + tool.s(r.getAs("RISK_CLASS")) + "|" + 
                     tool.s(r.getAs("RISK_FACTOR_SCENARIO_FAMILY")) + "|" + tool.s(r.getAs("RISK_BUCKET"));
        localCorrMap.computeIfAbsent(key, k -> new ArrayList<>()).add(r);
    }
    
    // 2. Broadcast using JavaSparkContext (Fixes the "cannot find symbol" error)
    // We wrap the raw context to use the Java-friendly broadcast method
    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());
    final Broadcast<Map<String, List<Row>>> corrBroadcast = jsc.broadcast(localCorrMap);

    // 3. Define Output Schema
    StructType outputSchema = DataTypes.createStructType(new StructField[]{
        DataTypes.createStructField("AGGREGATION_LEVEL", DataTypes.StringType, true),
        DataTypes.createStructField("AGGREGATION_LEVEL_VALUE", DataTypes.StringType, true),
        DataTypes.createStructField("REGIME", DataTypes.StringType, true),
        DataTypes.createStructField("REPORTING_ENTITY_COPER_ID", DataTypes.StringType, true),
        DataTypes.createStructField("RISK_BUCKET", DataTypes.StringType, true),
        DataTypes.createStructField("RISK_CLASS", DataTypes.StringType, true),
        DataTypes.createStructField("RISK_FACTOR_SCENARIO_FAMILY", DataTypes.StringType, true),
        DataTypes.createStructField("RISK_FACTOR_NAME_1", DataTypes.StringType, true),
        DataTypes.createStructField("EXCLUDED_RISK_BUCKET", DataTypes.StringType, true),
        DataTypes.createStructField("SUM_TERM_OFF_DIAGONAL_MEDIUM", DataTypes.DoubleType, false),
        DataTypes.createStructField("SUM_TERM_OFF_DIAGONAL_LOW", DataTypes.DoubleType, false),
        DataTypes.createStructField("SUM_TERM_OFF_DIAGONAL_HIGH", DataTypes.DoubleType, false),
        DataTypes.createStructField("MAX_WEIGHTED_SENSITIVITY_K", DataTypes.DoubleType, false)
    });

    // =========================================================================
    // MAIN EXECUTION PIPELINE
    // =========================================================================
    return inputDs
        .groupByKey((MapFunction<Row, String>) r -> 
            tool.s(r.getAs("REPORTING_ENTITY_COPER_ID")) + "|" + 
            tool.s(r.getAs("REGIME")) + "|" + 
            tool.s(r.getAs("RISK_BUCKET")), 
            Encoders.STRING())
        // FIX: Use FlatMapGroupsFunction explicitly with an Anonymous Inner Class
        .flatMapGroups(new FlatMapGroupsFunction<String, Row, Row>() {
            @Override
            public Iterator<Row> call(String groupKey, Iterator<Row> iterator) throws Exception {
                
                // POJO for performance
                class FactorNode {
                    final String name, issuer, excludedBucket;
                    final double sens;
                    FactorNode(Row r) {
                        this.name = tool.s(r.getAs("RISK_FACTOR_NAME"));
                        this.issuer = tool.extractIssuer(this.name);
                        this.excludedBucket = tool.s(r.getAs("EXCLUDED_RISK_BUCKET"));
                        this.sens = tool.d(r.getAs("WEIGHTED_SENSITIVITY"));
                    }
                }

                // Step 1: Materialize Input
                List<Row> metaHolder = new ArrayList<>();
                List<FactorNode> nodes = new ArrayList<>();
                while (iterator.hasNext()) {
                    Row r = iterator.next();
                    if (metaHolder.isEmpty()) metaHolder.add(r);
                    nodes.add(new FactorNode(r));
                }

                if (nodes.isEmpty()) return Collections.emptyIterator();
                Row metaRow = metaHolder.get(0);

                // Step 2 & 3 Setup: Context and Lookup
                String rClass = tool.s(metaRow.getAs("RISK_CLASS"));
                boolean isEquity = "EQUITY".equalsIgnoreCase(rClass) || "CSR_NON_SEC".equalsIgnoreCase(rClass);
                
                String lookupKey = tool.s(metaRow.getAs("REGIME")) + "|" + rClass + "|" + 
                                   tool.s(metaRow.getAs("RISK_FACTOR_SCENARIO_FAMILY")) + "|" + 
                                   tool.s(metaRow.getAs("RISK_BUCKET"));
                
                List<Row> rules = corrBroadcast.value().getOrDefault(lookupKey, Collections.emptyList());

                // Step 4: Parallel Calculation (Core Logic)
                return IntStream.range(0, nodes.size()).parallel().mapToObj(i -> {
                    FactorNode t1 = nodes.get(i);
                    double sumMed = 0.0, sumLow = 0.0, sumHigh = 0.0;

                    for (FactorNode t2 : nodes) {
                        // Logic: Diff Issuer?
                        String diffIssuer = "N";
                        if (isEquity && !t1.issuer.equals(t2.issuer)) {
                            diffIssuer = "Y";
                        }

                        // Logic: Lookup Rule
                        Row match = null;
                        for (Row rule : rules) {
                            if (!isEquity) {
                                if (tool.s(rule.getAs("RISK_FACTOR_NAME_1")).equals(t1.name) &&
                                    tool.s(rule.getAs("RISK_FACTOR_NAME_2")).equals(t2.name)) {
                                    match = rule; 
                                    break;
                                }
                            } else {
                                String marker = tool.s(rule.getAs("RISK_FACTOR_NAME_1")).trim();
                                if (!marker.isEmpty() && marker.startsWith(diffIssuer)) {
                                    match = rule;
                                    break;
                                }
                            }
                        }

                        // Logic: Aggregation
                        if (match != null) {
                            double term = t1.sens * t2.sens;
                            sumMed += tool.d(match.getAs("CORRELATION_MEDIUM")) * term;
                            sumLow += tool.d(match.getAs("CORRELATION_LOW")) * term;
                            sumHigh += tool.d(match.getAs("CORRELATION_HIGH")) * term;
                        }
                    }

                    // Build Result Row
                    return RowFactory.create(
                        tool.s(metaRow.getAs("AGGREGATION_LEVEL")),
                        tool.s(metaRow.getAs("AGGREGATION_LEVEL_VALUE")),
                        tool.s(metaRow.getAs("REGIME")),
                        tool.s(metaRow.getAs("REPORTING_ENTITY_COPER_ID")),
                        tool.s(metaRow.getAs("RISK_BUCKET")),
                        tool.s(metaRow.getAs("RISK_CLASS")),
                        tool.s(metaRow.getAs("RISK_FACTOR_SCENARIO_FAMILY")),
                        t1.name,
                        t1.excludedBucket,
                        sumMed, sumLow, sumHigh,
                        t1.sens
                    );
                }).collect(Collectors.toList()).iterator();
            }
        }, RowEncoder.apply(outputSchema));
}
