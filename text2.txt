import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapGroupsFunction;
import org.apache.spark.api.java.function.MapFunction;
import org.apache.spark.broadcast.Broadcast;
import org.apache.spark.sql.*;
import org.apache.spark.sql.catalyst.encoders.RowEncoder;
import org.apache.spark.sql.types.*;
import java.util.*;
import java.util.stream.*;
import java.io.Serializable; // 显式引入

/**
 * Main execution method containing all business logic.
 * Serializable-safe version: Removes captured local classes.
 */
public Dataset<Row> process(SparkSession spark, Map<String, Dataset<Row>> dsMap) {

    // =========================================================================
    // 1. Retrieve Inputs
    // =========================================================================
    Dataset<Row> inputDs = dsMap.get("SBM_DELTA_VEGA_SENSITIVITIES");
    Dataset<Row> correlationDs = dsMap.get("DELTA_VEGA_INTRA_BUCKET_RESULT_CORRELATION");

    if (inputDs == null || correlationDs == null) {
        throw new RuntimeException("Missing required inputs.");
    }

    // =========================================================================
    // 2. Prepare Correlation Map (Driver Side Logic Only)
    // =========================================================================
    // 这里我们使用内联逻辑，避免定义需要序列化的类
    Map<String, List<Row>> localCorrMap = new HashMap<>();
    List<Row> corrList = correlationDs.collectAsList();
    
    for (Row r : corrList) {
        // Driver-side safe null handling
        String reg = (r.getAs("REGIME") == null) ? "" : r.getAs("REGIME").toString().trim();
        String cls = (r.getAs("RISK_CLASS") == null) ? "" : r.getAs("RISK_CLASS").toString().trim();
        String fam = (r.getAs("RISK_FACTOR_SCENARIO_FAMILY") == null) ? "" : r.getAs("RISK_FACTOR_SCENARIO_FAMILY").toString().trim();
        String bkt = (r.getAs("RISK_BUCKET") == null) ? "" : r.getAs("RISK_BUCKET").toString().trim();
        
        String key = reg + "|" + cls + "|" + fam + "|" + bkt;
        localCorrMap.computeIfAbsent(key, k -> new ArrayList<>()).add(r);
    }
    
    // Broadcast the map
    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());
    final Broadcast<Map<String, List<Row>>> corrBroadcast = jsc.broadcast(localCorrMap);

    // =========================================================================
    // 3. Define Output Schema
    // =========================================================================
    StructType outputSchema = DataTypes.createStructType(new StructField[]{
        DataTypes.createStructField("AGGREGATION_LEVEL", DataTypes.StringType, true),
        DataTypes.createStructField("AGGREGATION_LEVEL_VALUE", DataTypes.StringType, true),
        DataTypes.createStructField("REGIME", DataTypes.StringType, true),
        DataTypes.createStructField("REPORTING_ENTITY_COPER_ID", DataTypes.StringType, true),
        DataTypes.createStructField("RISK_BUCKET", DataTypes.StringType, true),
        DataTypes.createStructField("RISK_CLASS", DataTypes.StringType, true),
        DataTypes.createStructField("RISK_FACTOR_SCENARIO_FAMILY", DataTypes.StringType, true),
        DataTypes.createStructField("RISK_FACTOR_NAME_1", DataTypes.StringType, true),
        DataTypes.createStructField("EXCLUDED_RISK_BUCKET", DataTypes.StringType, true),
        DataTypes.createStructField("SUM_TERM_OFF_DIAGONAL_MEDIUM", DataTypes.DoubleType, false),
        DataTypes.createStructField("SUM_TERM_OFF_DIAGONAL_LOW", DataTypes.DoubleType, false),
        DataTypes.createStructField("SUM_TERM_OFF_DIAGONAL_HIGH", DataTypes.DoubleType, false),
        DataTypes.createStructField("MAX_WEIGHTED_SENSITIVITY_K", DataTypes.DoubleType, false)
    });

    // =========================================================================
    // 4. Execution Pipeline (Safe from Serialization Errors)
    // =========================================================================
    return inputDs
        .groupByKey(new MapFunction<Row, String>() {
            @Override
            public String call(Row r) throws Exception {
                // INLINE LOGIC: Do not use external helper objects here.
                // This runs on Executors and must be self-contained.
                String id = (r.getAs("REPORTING_ENTITY_COPER_ID") == null) ? "" : r.getAs("REPORTING_ENTITY_COPER_ID").toString().trim();
                String reg = (r.getAs("REGIME") == null) ? "" : r.getAs("REGIME").toString().trim();
                String bkt = (r.getAs("RISK_BUCKET") == null) ? "" : r.getAs("RISK_BUCKET").toString().trim();
                
                return id + "|" + reg + "|" + bkt;
            }
        }, Encoders.STRING())
        .flatMapGroups(new FlatMapGroupsFunction<String, Row, Row>() {
            @Override
            public Iterator<Row> call(String groupKey, Iterator<Row> iterator) throws Exception {
                
                // --- EXECUTOR LOCAL CLASS DEFINITION ---
                // 定义在 call 内部，确保它是在 Executor 端创建的，无需从 Driver 序列化传输。
                class Tool {
                    String s(Object o) { return (o == null) ? "" : o.toString().trim(); }
                    double d(Object o) { return (o instanceof Number) ? ((Number) o).doubleValue() : 0.0; }
                    String extractIssuer(String name) {
                        if (name == null || !name.contains("_")) return name == null ? "" : name;
                        return name.substring(name.lastIndexOf("_") + 1);
                    }
                }
                final Tool tool = new Tool(); // Local instance

                // POJO for performance
                class FactorNode {
                    final String name, issuer, excludedBucket;
                    final double sens;
                    FactorNode(Row r) {
                        this.name = tool.s(r.getAs("RISK_FACTOR_NAME"));
                        this.issuer = tool.extractIssuer(this.name);
                        this.excludedBucket = tool.s(r.getAs("EXCLUDED_RISK_BUCKET"));
                        this.sens = tool.d(r.getAs("WEIGHTED_SENSITIVITY"));
                    }
                }

                // Step 1: Materialize Input
                List<Row> metaHolder = new ArrayList<>();
                List<FactorNode> nodes = new ArrayList<>();
                while (iterator.hasNext()) {
                    Row r = iterator.next();
                    if (metaHolder.isEmpty()) metaHolder.add(r);
                    nodes.add(new FactorNode(r));
                }

                if (nodes.isEmpty()) return Collections.emptyIterator();
                Row metaRow = metaHolder.get(0);

                // Step 2 & 3 Setup: Context and Lookup
                String rClass = tool.s(metaRow.getAs("RISK_CLASS"));
                boolean isEquity = "EQUITY".equalsIgnoreCase(rClass) || "CSR_NON_SEC".equalsIgnoreCase(rClass);
                
                String lookupKey = tool.s(metaRow.getAs("REGIME")) + "|" + rClass + "|" + 
                                   tool.s(metaRow.getAs("RISK_FACTOR_SCENARIO_FAMILY")) + "|" + 
                                   tool.s(metaRow.getAs("RISK_BUCKET"));
                
                // Retrieve rules from Broadcast (Safe, as Broadcast is designed to be serialized)
                List<Row> rules = corrBroadcast.value().getOrDefault(lookupKey, Collections.emptyList());

                // Step 4: Parallel Calculation
                return IntStream.range(0, nodes.size()).parallel().mapToObj(i -> {
                    FactorNode t1 = nodes.get(i);
                    double sumMed = 0.0, sumLow = 0.0, sumHigh = 0.0;

                    for (FactorNode t2 : nodes) {
                        // Logic: Diff Issuer?
                        String diffIssuer = "N";
                        if (isEquity && !t1.issuer.equals(t2.issuer)) {
                            diffIssuer = "Y";
                        }

                        // Logic: Lookup Rule
                        Row match = null;
                        for (Row rule : rules) {
                            if (!isEquity) {
                                if (tool.s(rule.getAs("RISK_FACTOR_NAME_1")).equals(t1.name) &&
                                    tool.s(rule.getAs("RISK_FACTOR_NAME_2")).equals(t2.name)) {
                                    match = rule; 
                                    break;
                                }
                            } else {
                                String marker = tool.s(rule.getAs("RISK_FACTOR_NAME_1")).trim();
                                if (!marker.isEmpty() && marker.startsWith(diffIssuer)) {
                                    match = rule;
                                    break;
                                }
                            }
                        }

                        // Logic: Aggregation
                        if (match != null) {
                            double term = t1.sens * t2.sens;
                            sumMed += tool.d(match.getAs("CORRELATION_MEDIUM")) * term;
                            sumLow += tool.d(match.getAs("CORRELATION_LOW")) * term;
                            sumHigh += tool.d(match.getAs("CORRELATION_HIGH")) * term;
                        }
                    }

                    // Build Result Row
                    return RowFactory.create(
                        tool.s(metaRow.getAs("AGGREGATION_LEVEL")),
                        tool.s(metaRow.getAs("AGGREGATION_LEVEL_VALUE")),
                        tool.s(metaRow.getAs("REGIME")),
                        tool.s(metaRow.getAs("REPORTING_ENTITY_COPER_ID")),
                        tool.s(metaRow.getAs("RISK_BUCKET")),
                        tool.s(metaRow.getAs("RISK_CLASS")),
                        tool.s(metaRow.getAs("RISK_FACTOR_SCENARIO_FAMILY")),
                        t1.name,
                        t1.excludedBucket,
                        sumMed, sumLow, sumHigh,
                        t1.sens
                    );
                }).collect(Collectors.toList()).iterator();
            }
        }, RowEncoder.apply(outputSchema));
}
