import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapGroupsFunction;
import org.apache.spark.api.java.function.MapFunction;
import org.apache.spark.broadcast.Broadcast;
import org.apache.spark.sql.*;
import org.apache.spark.sql.catalyst.encoders.RowEncoder;
import org.apache.spark.sql.types.*;

// Necessary Java Standard Imports
import java.util.*;
import java.util.stream.*;
import java.util.function.IntFunction; 
import java.io.Serializable; // Critical for explicit serialization

public Dataset<Row> process(SparkSession spark, Map<String, Dataset<Row>> dsMap) {

    // =========================================================================
    // 1. Retrieve Inputs
    // =========================================================================
    Dataset<Row> inputDs = dsMap.get("SBM_DELTA_VEGA_SENSITIVITIES");
    Dataset<Row> correlationDs = dsMap.get("DELTA_VEGA_INTRA_BUCKET_RESULT_CORRELATION");

    if (inputDs == null || correlationDs == null) {
        throw new RuntimeException("Missing required inputs.");
    }

    // =========================================================================
    // 2. Prepare Broadcast (Driver Side)
    // =========================================================================
    Map<String, List<Row>> localCorrMap = new HashMap<>();
    List<Row> corrList = correlationDs.collectAsList();
    
    for (Row r : corrList) {
        String reg = (r.getAs("REGIME") == null) ? "" : r.getAs("REGIME").toString().trim();
        String cls = (r.getAs("RISK_CLASS") == null) ? "" : r.getAs("RISK_CLASS").toString().trim();
        String fam = (r.getAs("RISK_FACTOR_SCENARIO_FAMILY") == null) ? "" : r.getAs("RISK_FACTOR_SCENARIO_FAMILY").toString().trim();
        String bkt = (r.getAs("RISK_BUCKET") == null) ? "" : r.getAs("RISK_BUCKET").toString().trim();
        
        String key = reg + "|" + cls + "|" + fam + "|" + bkt;
        
        if (!localCorrMap.containsKey(key)) {
            localCorrMap.put(key, new ArrayList<>());
        }
        localCorrMap.get(key).add(r);
    }
    
    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());
    final Broadcast<Map<String, List<Row>>> corrBroadcast = jsc.broadcast(localCorrMap);

    // =========================================================================
    // 3. Define Output Schema
    // =========================================================================
    StructType outputSchema = DataTypes.createStructType(new StructField[]{
        DataTypes.createStructField("AGGREGATION_LEVEL", DataTypes.StringType, true),
        DataTypes.createStructField("AGGREGATION_LEVEL_VALUE", DataTypes.StringType, true),
        DataTypes.createStructField("REGIME", DataTypes.StringType, true),
        DataTypes.createStructField("REPORTING_ENTITY_COPER_ID", DataTypes.StringType, true),
        DataTypes.createStructField("RISK_BUCKET", DataTypes.StringType, true),
        DataTypes.createStructField("RISK_CLASS", DataTypes.StringType, true),
        DataTypes.createStructField("RISK_FACTOR_SCENARIO_FAMILY", DataTypes.StringType, true),
        DataTypes.createStructField("RISK_FACTOR_NAME_1", DataTypes.StringType, true),
        DataTypes.createStructField("EXCLUDED_RISK_BUCKET", DataTypes.StringType, true),
        DataTypes.createStructField("SUM_TERM_OFF_DIAGONAL_MEDIUM", DataTypes.DoubleType, false),
        DataTypes.createStructField("SUM_TERM_OFF_DIAGONAL_LOW", DataTypes.DoubleType, false),
        DataTypes.createStructField("SUM_TERM_OFF_DIAGONAL_HIGH", DataTypes.DoubleType, false),
        DataTypes.createStructField("MAX_WEIGHTED_SENSITIVITY_K", DataTypes.DoubleType, false)
    });

    // =========================================================================
    // 4. Execution Pipeline (Detached Local Class Pattern)
    // =========================================================================
    return inputDs
        .groupByKey(new MapFunction<Row, String>() {
            @Override
            public String call(Row r) throws Exception {
                String id = (r.getAs("REPORTING_ENTITY_COPER_ID") == null) ? "" : r.getAs("REPORTING_ENTITY_COPER_ID").toString().trim();
                String reg = (r.getAs("REGIME") == null) ? "" : r.getAs("REGIME").toString().trim();
                String bkt = (r.getAs("RISK_BUCKET") == null) ? "" : r.getAs("RISK_BUCKET").toString().trim();
                return id + "|" + reg + "|" + bkt;
            }
        }, Encoders.STRING())
        
        .flatMapGroups(new FlatMapGroupsFunction<String, Row, Row>() {
            @Override
            public Iterator<Row> call(String key, Iterator<Row> iterator) throws Exception {
                
                // ---------------------------------------------------------
                // A. Define Serializable Helper Classes
                //    (Explicitly implementing Serializable)
                // ---------------------------------------------------------
                
                // Tool: Pure logic, serializable
                class Tool implements Serializable {
                    String s(Object o) { return (o == null) ? "" : o.toString().trim(); }
                    double d(Object o) { return (o instanceof Number) ? ((Number) o).doubleValue() : 0.0; }
                    String extractIssuer(String name) {
                        if (name == null || !name.contains("_")) return name == null ? "" : name;
                        return name.substring(name.lastIndexOf("_") + 1);
                    }
                }
                final Tool tool = new Tool();

                // FactorNode: Data container, serializable
                class FactorNode implements Serializable {
                    final String name, issuer, excludedBucket;
                    final double sens;
                    FactorNode(Row r) {
                        this.name = tool.s(r.getAs("RISK_FACTOR_NAME"));
                        this.issuer = tool.extractIssuer(this.name);
                        this.excludedBucket = tool.s(r.getAs("EXCLUDED_RISK_BUCKET"));
                        this.sens = tool.d(r.getAs("WEIGHTED_SENSITIVITY"));
                    }
                }

                // ---------------------------------------------------------
                // B. Data Materialization
                // ---------------------------------------------------------
                List<Row> metaHolder = new ArrayList<>();
                List<FactorNode> nodes = new ArrayList<>();
                while (iterator.hasNext()) {
                    Row r = iterator.next();
                    if (metaHolder.isEmpty()) metaHolder.add(r);
                    nodes.add(new FactorNode(r));
                }

                if (nodes.isEmpty()) return Collections.emptyIterator();
                final Row metaRow = metaHolder.get(0);

                // ---------------------------------------------------------
                // C. Prepare Context Data
                // ---------------------------------------------------------
                String rClass = tool.s(metaRow.getAs("RISK_CLASS"));
                boolean isEquity = "EQUITY".equalsIgnoreCase(rClass) || "CSR_NON_SEC".equalsIgnoreCase(rClass);
                
                String lookupKey = tool.s(metaRow.getAs("REGIME")) + "|" + rClass + "|" + 
                                   tool.s(metaRow.getAs("RISK_FACTOR_SCENARIO_FAMILY")) + "|" + 
                                   tool.s(metaRow.getAs("RISK_BUCKET"));
                
                List<Row> rules = corrBroadcast.value().getOrDefault(lookupKey, Collections.emptyList());

                // ---------------------------------------------------------
                // D. The "Detached" RowMapper Class
                //    (This is the fix: A named local class that takes state via constructor)
                // ---------------------------------------------------------
                class RowMapper implements IntFunction<Row>, Serializable {
                    // All state is passed in explicitly, preventing implicit outer class capture
                    private final List<FactorNode> nodes;
                    private final List<Row> rules;
                    private final Row metaRow;
                    private final Tool tool;
                    private final boolean isEquity;

                    public RowMapper(List<FactorNode> nodes, List<Row> rules, Row metaRow, Tool tool, boolean isEquity) {
                        this.nodes = nodes;
                        this.rules = rules;
                        this.metaRow = metaRow;
                        this.tool = tool;
                        this.isEquity = isEquity;
                    }

                    @Override
                    public Row apply(int i) {
                        FactorNode t1 = nodes.get(i);
                        double sumMed = 0.0, sumLow = 0.0, sumHigh = 0.0;

                        for (FactorNode t2 : nodes) {
                            // Logic: Issuer Diff
                            String diffIssuer = "N";
                            if (isEquity && !t1.issuer.equals(t2.issuer)) {
                                diffIssuer = "Y";
                            }

                            // Logic: Lookup
                            Row match = null;
                            for (Row rule : rules) {
                                if (!isEquity) {
                                    if (tool.s(rule.getAs("RISK_FACTOR_NAME_1")).equals(t1.name) &&
                                        tool.s(rule.getAs("RISK_FACTOR_NAME_2")).equals(t2.name)) {
                                        match = rule; 
                                        break;
                                    }
                                } else {
                                    String marker = tool.s(rule.getAs("RISK_FACTOR_NAME_1")).trim();
                                    if (!marker.isEmpty() && marker.startsWith(diffIssuer)) {
                                        match = rule;
                                        break;
                                    }
                                }
                            }

                            // Logic: Aggregate
                            if (match != null) {
                                double term = t1.sens * t2.sens;
                                sumMed += tool.d(match.getAs("CORRELATION_MEDIUM")) * term;
                                sumLow += tool.d(match.getAs("CORRELATION_LOW")) * term;
                                sumHigh += tool.d(match.getAs("CORRELATION_HIGH")) * term;
                            }
                        }

                        return RowFactory.create(
                            tool.s(metaRow.getAs("AGGREGATION_LEVEL")),
                            tool.s(metaRow.getAs("AGGREGATION_LEVEL_VALUE")),
                            tool.s(metaRow.getAs("REGIME")),
                            tool.s(metaRow.getAs("REPORTING_ENTITY_COPER_ID")),
                            tool.s(metaRow.getAs("RISK_BUCKET")),
                            tool.s(metaRow.getAs("RISK_CLASS")),
                            tool.s(metaRow.getAs("RISK_FACTOR_SCENARIO_FAMILY")),
                            t1.name,
                            t1.excludedBucket,
                            sumMed, sumLow, sumHigh,
                            t1.sens
                        );
                    }
                }

                // ---------------------------------------------------------
                // E. Parallel Execution using the Named Mapper
                // ---------------------------------------------------------
                // Pass all dependencies explicitly to the constructor
                RowMapper mapper = new RowMapper(nodes, rules, metaRow, tool, isEquity);

                return IntStream.range(0, nodes.size())
                    .parallel()
                    .mapToObj(mapper) // Use the safe, detached mapper instance
                    .collect(Collectors.toList())
                    .iterator();
            }
        }, RowEncoder.apply(outputSchema));
}
