FX risk refers to the risk associated with fluctuations in foreign exchange rates. 
In the context of SA-CVA, FX risk factors are used to determine the potential credit exposure of a derivative transaction due to changes in exchange rates.

Deal(Row r) {
                // [FIX]: Use String.valueOf() to handle cases where ID fields are Integers in the data
                this.AggLevel = safeString(r.getAs("AggLevel"));
                this.AggLevelValue = safeString(r.getAs("AggLevelValue"));
                this.CreditQuality = safeString(r.getAs("CreditQuality"));
                this.Regime = safeString(r.getAs("Regime"));
                
                // [FIX]: Specifically fixing RFBucket cast error here
                this.RFBucket = safeString(r.getAs("RFBucket"));
                
                this.RFTenor = safeString(r.getAs("RFTenor"));
                this.RiskClass = safeString(r.getAs("RiskClass"));
                this.RiskWeightDerivingCoperID = safeString(r.getAs("RiskWeightDerivingCoperID"));
                this.ScenarioType = safeString(r.getAs("ScenarioType"));
                this.UltimateParentIdentifier = safeString(r.getAs("UltimateParentIdentifier"));

                // Parse Boolean safely
                Object idxObj = r.getAs("IsQualifiedIndex");
                this.IsQualifiedIndex = (idxObj instanceof Boolean) 
                    ? (Boolean) idxObj 
                    : Boolean.parseBoolean(String.valueOf(idxObj));

                // Parse Numeric fields safely
                this.RiskWeightedCvaSensitivity = parseDoubleSafe(r.getAs("RiskWeightedCvaSensitivity"));
                this.RiskWeightedHedgeSensitivity = parseDoubleSafe(r.getAs("RiskWeightedHedgeSensitivity"));
                this.RiskWeightedNetSensitivity = parseDoubleSafe(r.getAs("RiskWeightedNetSensitivity"));
            }

            // Helper: Safely convert any Object (Integer, Long, String) to String
            private String safeString(Object val) {
                return val == null ? null : String.valueOf(val);
            }

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.Map;
import java.util.Objects;
import java.util.concurrent.atomic.DoubleAdder;
import java.util.stream.IntStream;


class Deal {
            // Attributes matching the input Schema
            final String AggLevel;
            final String AggLevelValue;
            final String CreditQuality;
            final String Regime;
            final String RFBucket;
            final String RFTenor;
            final String RiskClass;
            final String RiskWeightDerivingCoperID;
            final String ScenarioType;
            final String UltimateParentIdentifier;
            final boolean IsQualifiedIndex;
            
            // Numeric Sensitivities
            final double RiskWeightedCvaSensitivity;
            final double RiskWeightedHedgeSensitivity;
            final double RiskWeightedNetSensitivity;

            // Constructor: Parses Row data safely
            Deal(Row r) {
                this.AggLevel = r.getAs("AggLevel");
                this.AggLevelValue = r.getAs("AggLevelValue");
                this.CreditQuality = r.getAs("CreditQuality");
                this.Regime = r.getAs("Regime");
                this.RFBucket = r.getAs("RFBucket");
                this.RFTenor = r.getAs("RFTenor");
                this.RiskClass = r.getAs("RiskClass");
                this.RiskWeightDerivingCoperID = r.getAs("RiskWeightDerivingCoperID");
                this.ScenarioType = r.getAs("ScenarioType");
                this.UltimateParentIdentifier = r.getAs("UltimateParentIdentifier");

                // Parse Boolean safely
                Object idxObj = r.getAs("IsQualifiedIndex");
                this.IsQualifiedIndex = (idxObj instanceof Boolean) 
                    ? (Boolean) idxObj 
                    : Boolean.parseBoolean(String.valueOf(idxObj));

                // Parse Numeric fields safely (handles BigDecimal/Double/String)
                this.RiskWeightedCvaSensitivity = parseDoubleSafe(r.getAs("RiskWeightedCvaSensitivity"));
                this.RiskWeightedHedgeSensitivity = parseDoubleSafe(r.getAs("RiskWeightedHedgeSensitivity"));
                this.RiskWeightedNetSensitivity = parseDoubleSafe(r.getAs("RiskWeightedNetSensitivity"));
            }

            // Helper to handle Spark numeric type polymorphism
            private double parseDoubleSafe(Object val) {
                if (val == null) return 0.0;
                if (val instanceof Number) return ((Number) val).doubleValue();
                try {
                    return Double.parseDouble(val.toString());
                } catch (NumberFormatException e) {
                    return 0.0;
                }
            }

            // Correlation Logic: Calculates 'rho' between this deal and another deal
            double correlationWith(Deal other) {
                // Rule 1: Identity Check
                // If attributes are strictly identical, return 0.0 to exclude self-correlation 
                // in the cross-term loop (though the loop j>i handles index, this handles duplicates).
                if (Objects.equals(this.RiskWeightDerivingCoperID, other.RiskWeightDerivingCoperID) &&
                    Objects.equals(this.RFTenor, other.RFTenor) &&
                    Objects.equals(this.CreditQuality, other.CreditQuality) &&
                    Objects.equals(this.UltimateParentIdentifier, other.UltimateParentIdentifier)) {
                    return 0.0;
                }

                double rho = 1.0;

                // Rule 2: Tenor Mismatch
                if (!Objects.equals(this.RFTenor, other.RFTenor)) {
                    rho *= 0.9;
                }

                // Rule 3: Credit Quality Mismatch
                if (!Objects.equals(this.CreditQuality, other.CreditQuality)) {
                    rho *= 0.8;
                }

                // Rule 4: Issuer / Parent Relationship
                if (Objects.equals(this.RiskWeightDerivingCoperID, other.RiskWeightDerivingCoperID)) {
                    // Same Issuer -> 1.0
                } 
                else if (Objects.equals(this.UltimateParentIdentifier, other.UltimateParentIdentifier) 
                         && this.UltimateParentIdentifier != null) {
                    // Same Parent (Different Issuer) -> 0.9
                    rho *= 0.9;
                } 
                else {
                    // Unrelated -> 0.5
                    rho *= 0.5;
                }
                return rho;
            }
        }

        // =====================================================================
        // 2. Data Ingestion (Spark -> Java Heap)
        // =====================================================================
        // Attempt to find the input dataset (defaulting to "INPUT_DATA" or first available)
        String inputKey = dsMap.containsKey("INPUT_DATA") ? "INPUT_DATA" : dsMap.keySet().stream().findFirst().orElse(null);
        if (inputKey == null) throw new RuntimeException("No Input Dataset found in dsMap");

        Dataset<Row> inputDataset = dsMap.get(inputKey);
        List<Row> rows = inputDataset.collectAsList();

        // Convert Rows to Deal objects
        List<Deal> deals = new ArrayList<>(rows.size());
        for (Row r : rows) {
            deals.add(new Deal(r));
        }

        // =====================================================================
        // 3. Core Calculation (High Performance Kernel)
        // =====================================================================
        Deal[] nodes = deals.toArray(new Deal[0]);
        int N = nodes.length;

        // Accumulators for the three specific terms
        DoubleAdder accPoweredNetRws = new DoubleAdder();             // Term 1
        DoubleAdder accSumSumCorrelatedSensitivity = new DoubleAdder(); // Term 2
        DoubleAdder accPoweredHedgeRws = new DoubleAdder();           // Term 3

        // Parallel Stream Execution
        IntStream.range(0, N).parallel().forEach(i -> {
            Deal r1 = nodes[i];

            // Term 1: PoweredNetRws (Sum of Squared Net Sensitivities)
            // Formula: Sum(WS_net^2)
            accPoweredNetRws.add(r1.RiskWeightedNetSensitivity * r1.RiskWeightedNetSensitivity);

            // Term 3: SUM_PoweredHedgeRws (Sum of Squared Hedge Sensitivities)
            // Formula: Sum(WS_hedge^2)
            // Note: The R factor (0.01) is usually applied later or here depending on requirement.
            // Based on the variable name "SUM_Powered...", we calculate the raw sum of squares here.
            accPoweredHedgeRws.add(r1.RiskWeightedHedgeSensitivity * r1.RiskWeightedHedgeSensitivity);

            // Term 2: SUM_SUM_CorrelatedSensitivity (Cross Terms)
            // Formula: Sum(Sum(Rho * WS_k * WS_l)) where k != l
            double localCrossSum = 0.0;
            
            // Optimization: Iterate only Upper Triangle (j > i)
            for (int j = i + 1; j < N; j++) {
                Deal r2 = nodes[j];
                
                double rho = r1.correlationWith(r2);
                
                // Multiply by 2.0 to account for the symmetric Lower Triangle (i > j)
                localCrossSum += (r1.RiskWeightedNetSensitivity * r2.RiskWeightedNetSensitivity * rho) * 2.0;
            }
            
            if (localCrossSum != 0.0) {
                accSumSumCorrelatedSensitivity.add(localCrossSum);
            }
        });

        // =====================================================================
        // 4. Result Packaging
        // =====================================================================
        double resultTerm1 = accPoweredNetRws.sum();
        double resultTerm2 = accSumSumCorrelatedSensitivity.sum();
        double resultTerm3 = accPoweredHedgeRws.sum();

        // Create a single row with the 3 terms
        Row resultRow = RowFactory.create(resultTerm1, resultTerm2, resultTerm3);
        List<Row> resultList = Collections.singletonList(resultRow);

        // Define Output Schema
        StructType schema = DataTypes.createStructType(new StructField[]{
            DataTypes.createStructField("PoweredNetRws", DataTypes.DoubleType, false),
            DataTypes.createStructField("SUM_SUM_CorrelatedSensitivity", DataTypes.DoubleType, false),
            DataTypes.createStructField("SUM_PoweredHedgeRws", DataTypes.DoubleType, false)
        });

        return spark.createDataFrame(resultList, schema);
    }


























import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import java.util.List;
import java.util.ArrayList;
import java.util.Objects;
import java.util.concurrent.atomic.DoubleAdder;
import java.util.stream.IntStream;

public class DirectSparkRiskEngine {

    // REPLACEMENT: Use a static class instead of 'record' for Java 8/11 compatibility.
    // We use 'public final' fields to allow direct access (e.g., deal.RFTenor) similar to struct.
    public static class Deal {
        public final String RiskWeightDerivingCoperID;
        public final String UltimateParentIdentifier;
        public final String RFTenor;
        public final String CreditQuality;
        public final boolean IsQualifiedIndex;
        public final double RiskWeightedNetSensitivity;

        public Deal(
            String RiskWeightDerivingCoperID,
            String UltimateParentIdentifier,
            String RFTenor,
            String CreditQuality,
            boolean IsQualifiedIndex,
            double RiskWeightedNetSensitivity
        ) {
            this.RiskWeightDerivingCoperID = RiskWeightDerivingCoperID;
            this.UltimateParentIdentifier = UltimateParentIdentifier;
            this.RFTenor = RFTenor;
            this.CreditQuality = CreditQuality;
            this.IsQualifiedIndex = IsQualifiedIndex;
            this.RiskWeightedNetSensitivity = RiskWeightedNetSensitivity;
        }
    }

    /**
     * Public Interface
     */
    public double run(Dataset<Row> inputDataset) {
        // Step 1: Collect data
        List<Row> rows = inputDataset.collectAsList();
        
        // Step 2: Convert to Java Objects (Deal Class)
        List<Deal> records = new ArrayList<>(rows.size());
        for (Row r : rows) {
            records.add(new Deal(
                r.getAs("RiskWeightDerivingCoperID"),
                r.getAs("UltimateParentIdentifier"),
                r.getAs("RFTenor"),
                r.getAs("CreditQuality"),
                // Handle Boolean parsing safety
                r.get(r.fieldIndex("IsQualifiedIndex")) instanceof Boolean ? 
                    r.getAs("IsQualifiedIndex") : Boolean.parseBoolean(r.getAs("IsQualifiedIndex").toString()),
                // Handle Number parsing safety (Double/BigDecimal)
                ((Number) r.getAs("RiskWeightedNetSensitivity")).doubleValue() 
            ));
        }

        // Step 3: Calculate
        return calculateCore(records);
    }

    /**
     * Core Calculation Logic
     */
    private double calculateCore(List<Deal> recordList) {
        // Convert List to Array for performance
        Deal[] nodes = recordList.toArray(new Deal[0]);
        int N = nodes.length;

        DoubleAdder totalVariance = new DoubleAdder();

        // Parallel Calculation
        IntStream.range(0, N).parallel().forEach(i -> {
            Deal r1 = nodes[i];
            double localSum = 0.0;

            // Symmetric Matrix Optimization (j > i)
            for (int j = i + 1; j < N; j++) {
                Deal r2 = nodes[j];

                double rho = getCorrelation(r1, r2);

                // Covariance * 2 (Symmetric)
                localSum += (r1.RiskWeightedNetSensitivity * r2.RiskWeightedNetSensitivity * rho) * 2.0;
            }

            if (localSum != 0.0) {
                totalVariance.add(localSum);
            }
        });

        return Math.sqrt(totalVariance.sum());
    }

    /**
     * Business Rules (Correlation Logic)
     */
    private double getCorrelation(Deal t1, Deal t2) {
        
        // Rule 1: Identical Check
        if (Objects.equals(t1.RiskWeightDerivingCoperID, t2.RiskWeightDerivingCoperID) &&
            Objects.equals(t1.RFTenor, t2.RFTenor) &&
            Objects.equals(t1.CreditQuality, t2.CreditQuality) &&
            Objects.equals(t1.UltimateParentIdentifier, t2.UltimateParentIdentifier)) {
            return 0.0;
        }

        double rho = 1.0;

        // Rule 2: Tenor
        if (!Objects.equals(t1.RFTenor, t2.RFTenor)) {
            rho *= 0.9;
        }

        // Rule 3: Credit Quality
        if (!Objects.equals(t1.CreditQuality, t2.CreditQuality)) {
            rho *= 0.8;
        }

        // Rule 4: Issuer / Parent
        if (Objects.equals(t1.RiskWeightDerivingCoperID, t2.RiskWeightDerivingCoperID)) {
            // Same Issuer -> 1.0
        } 
        else if (Objects.equals(t1.UltimateParentIdentifier, t2.UltimateParentIdentifier) 
                 && t1.UltimateParentIdentifier != null) {
            // Same Parent -> 0.9
            rho *= 0.9;
        } 
        else {
            // Different -> 0.5
            rho *= 0.5;
        }

        return rho;
    }
}




-----------------------

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import java.util.List;
import java.util.ArrayList;
import java.util.Objects;
import java.util.concurrent.atomic.DoubleAdder;
import java.util.stream.IntStream;

public class DirectSparkRiskEngine {

    // Constant R for the Hedge Disallowance Term (Standard Basel III value is 0.01)
    private static final double R_FACTOR = 0.01;

    // Static class acting as a data carrier
    // Updated to include ALL attributes from the schema image
    public static class Deal {
        // String Fields
        public final String AggLevel;
        public final String AggLevelValue;
        public final String CreditQuality;
        public final String Regime;
        public final String RFBucket;
        public final String RFTenor;
        public final String RiskClass;
        public final String RiskWeightDerivingCoperID;
        public final String ScenarioType;
        public final String UltimateParentIdentifier;
        
        // Boolean Field
        public final boolean IsQualifiedIndex;
        
        // Numeric Fields (Sensitivities)
        public final double RiskWeightedCvaSensitivity;
        public final double RiskWeightedHedgeSensitivity;
        public final double RiskWeightedNetSensitivity;

        public Deal(
            String AggLevel,
            String AggLevelValue,
            String CreditQuality,
            String Regime,
            String RFBucket,
            String RFTenor,
            String RiskClass,
            String RiskWeightDerivingCoperID,
            String ScenarioType,
            String UltimateParentIdentifier,
            boolean IsQualifiedIndex,
            double RiskWeightedCvaSensitivity,
            double RiskWeightedHedgeSensitivity,
            double RiskWeightedNetSensitivity
        ) {
            this.AggLevel = AggLevel;
            this.AggLevelValue = AggLevelValue;
            this.CreditQuality = CreditQuality;
            this.Regime = Regime;
            this.RFBucket = RFBucket;
            this.RFTenor = RFTenor;
            this.RiskClass = RiskClass;
            this.RiskWeightDerivingCoperID = RiskWeightDerivingCoperID;
            this.ScenarioType = ScenarioType;
            this.UltimateParentIdentifier = UltimateParentIdentifier;
            this.IsQualifiedIndex = IsQualifiedIndex;
            this.RiskWeightedCvaSensitivity = RiskWeightedCvaSensitivity;
            this.RiskWeightedHedgeSensitivity = RiskWeightedHedgeSensitivity;
            this.RiskWeightedNetSensitivity = RiskWeightedNetSensitivity;
        }
    }

    /**
     * Public Interface: Accepts Spark Dataset
     */
    public double run(Dataset<Row> inputDataset) {
        // Step 1: Collect data from Spark to Driver memory
        List<Row> rows = inputDataset.collectAsList();
        
        // Step 2: Convert to Java Objects (Deal Class) with FULL attributes
        List<Deal> records = new ArrayList<>(rows.size());
        for (Row r : rows) {
            records.add(new Deal(
                r.getAs("AggLevel"),
                r.getAs("AggLevelValue"),
                r.getAs("CreditQuality"),
                r.getAs("Regime"),
                r.getAs("RFBucket"),
                r.getAs("RFTenor"),
                r.getAs("RiskClass"),
                r.getAs("RiskWeightDerivingCoperID"),
                r.getAs("ScenarioType"),
                r.getAs("UltimateParentIdentifier"),
                
                // Handle Boolean parsing safety
                r.get(r.fieldIndex("IsQualifiedIndex")) instanceof Boolean ? 
                    r.getAs("IsQualifiedIndex") : Boolean.parseBoolean(r.getAs("IsQualifiedIndex").toString()),
                
                // Handle Numeric parsing safety for all sensitivity columns
                getDouble(r, "RiskWeightedCvaSensitivity"),
                getDouble(r, "RiskWeightedHedgeSensitivity"),
                getDouble(r, "RiskWeightedNetSensitivity")
            ));
        }

        // Step 3: Calculate Kb
        return calculateKb(records);
    }

    /**
     * Helper to safely extract double from Spark Row regardless of underlying type (BigDecimal, Double, String)
     */
    private double getDouble(Row r, String colName) {
        Object val = r.getAs(colName);
        if (val == null) return 0.0;
        if (val instanceof Number) {
            return ((Number) val).doubleValue();
        }
        try {
            return Double.parseDouble(val.toString());
        } catch (NumberFormatException e) {
            return 0.0;
        }
    }

    /**
     * Core Calculation Logic for Kb
     * Formula: Sqrt( (Sum(WS^2) + Sum(Sum(Rho * WS * WS))) + R * Sum((WS_Hdg)^2) )
     */
    private double calculateKb(List<Deal> recordList) {
        Deal[] nodes = recordList.toArray(new Deal[0]);
        int N = nodes.length;

        // Term 1: Sum of Squared Net Sensitivities (Diagonal)
        DoubleAdder term1_DiagonalSum = new DoubleAdder();
        
        // Term 2: Correlated Sensitivities (Cross Terms)
        DoubleAdder term2_CrossSum = new DoubleAdder();

        // Term 3: Sum of Squared Hedge Sensitivities
        DoubleAdder term3_HedgeSum = new DoubleAdder();

        // Parallel Calculation
        IntStream.range(0, N).parallel().forEach(i -> {
            Deal r1 = nodes[i];

            // --- Term 1: Sum(WS_k^2) ---
            double netSensSq = r1.RiskWeightedNetSensitivity * r1.RiskWeightedNetSensitivity;
            term1_DiagonalSum.add(netSensSq);

            // --- Term 3: Sum((WS_k_Hdg)^2) ---
            double hedgeSensSq = r1.RiskWeightedHedgeSensitivity * r1.RiskWeightedHedgeSensitivity;
            term3_HedgeSum.add(hedgeSensSq);

            // --- Term 2: Sum(Sum(Rho * WS_k * WS_l)) ---
            // Calculate Upper Triangle (j > i) and multiply by 2
            double localCrossSum = 0.0;
            for (int j = i + 1; j < N; j++) {
                Deal r2 = nodes[j];

                double rho = getCorrelation(r1, r2);

                // Cross terms use Net Sensitivities
                localCrossSum += (r1.RiskWeightedNetSensitivity * r2.RiskWeightedNetSensitivity * rho) * 2.0;
            }

            if (localCrossSum != 0.0) {
                term2_CrossSum.add(localCrossSum);
            }
        });

        // Assemble the final formula components
        double componentA = term1_DiagonalSum.sum() + term2_CrossSum.sum(); // (Diagonal + Cross)
        double componentB = R_FACTOR * term3_HedgeSum.sum();                // R * Sum(Hedge^2)

        // Final Result: Sqrt( A + B )
        return Math.sqrt(Math.max(0.0, componentA + componentB));
    }

    /**
     * Business Rules (Correlation Logic)
     */
    private double getCorrelation(Deal t1, Deal t2) {
        
        // Rule 1: Identical Check
        if (Objects.equals(t1.RiskWeightDerivingCoperID, t2.RiskWeightDerivingCoperID) &&
            Objects.equals(t1.RFTenor, t2.RFTenor) &&
            Objects.equals(t1.CreditQuality, t2.CreditQuality) &&
            Objects.equals(t1.UltimateParentIdentifier, t2.UltimateParentIdentifier)) {
            return 0.0;
        }

        double rho = 1.0;

        // Rule 2: Tenor Correlation
        if (!Objects.equals(t1.RFTenor, t2.RFTenor)) {
            rho *= 0.9;
        }

        // Rule 3: Credit Quality Correlation
        if (!Objects.equals(t1.CreditQuality, t2.CreditQuality)) {
            rho *= 0.8;
        }

        // Rule 4: Issuer / Parent Correlation
        if (Objects.equals(t1.RiskWeightDerivingCoperID, t2.RiskWeightDerivingCoperID)) {
            // Same Issuer -> 1.0
        } 
        else if (Objects.equals(t1.UltimateParentIdentifier, t2.UltimateParentIdentifier) 
                 && t1.UltimateParentIdentifier != null) {
            // Same Parent -> 0.9
            rho *= 0.9;
        } 
        else {
            // Different -> 0.5
            rho *= 0.5;
        }

        return rho;
    }
}
