你的分析完全正确。根据你提供的 SQL 截图，在 `JOIN ... USING (...)` 子句中，连接条件确实包含以下字段：
`REPORTING_ENTITY_COPER_ID`, `REGIME`, `AGGREGATION_LEVEL`, `AGGREGATION_LEVEL_VALUE`, `RISK_CLASS`, `RISK_FACTOR_SCENARIO_FAMILY`, `RISK_BUCKET`。

这意味着只有当这 **7个字段** 全部相同时，行与行之间才应该进行 Self-Join。遗漏它们会导致不同 `AGGREGATION_LEVEL` 的数据错误地混在一起计算。

以下是修正后的代码：

1. **Step 1 Grouping Key**: 补充了 `AGGREGATION_LEVEL`, `AGGREGATION_LEVEL_VALUE` 和 `REPORTING_ENTITY_COPER_ID`。
2. **移除 Helper**: 移除了 `getSafeString`，改用内联的 `r.isNullAt(i) ? "" : r.get(i).toString().trim()` 逻辑，既安全又符合你的要求。
3. **注释**: 全程英文。

```java
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.broadcast.Broadcast;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;
import scala.Tuple2;

import java.util.*;
import java.util.concurrent.atomic.DoubleAdder;
import java.util.stream.IntStream;

public class SbmStructuredProcessor {

    public Dataset<Row> process(SparkSession sparkSession, Map<String, Dataset<Row>> dsMap) {
        // [Safety Check] Validate input datasets
        Dataset<Row> inputDs = dsMap.get("SBM_DELTA_VEGA_SENSITIVITIES");
        Dataset<Row> corrDs = dsMap.get("DELTA_VEGA_INTRA_BUCKET_RESULT_CORRELATION");

        if (inputDs == null || corrDs == null) throw new RuntimeException("Missing required datasets");

        // =====================================================================
        // 1. Broadcast Preparation (Driver Side)
        // =====================================================================
        // Select necessary columns from Correlation dataset
        List<Row> corrRows = corrDs.select(
            "REGIME", "RISK_CLASS", "RISK_FACTOR_SCENARIO_FAMILY", "RISK_BUCKET",
            "RISK_FACTOR_NAME_1", "RISK_FACTOR_NAME_2",
            "CORRELATION_MEDIUM", "CORRELATION_LOW", "CORRELATION_HIGH"
        ).collectAsList();

        Map<String, double[]> correlationMap = new HashMap<>(corrRows.size());
        for (Row r : corrRows) {
            // Build the compound key for the correlation map
            // Logic: Inline null checks + toString() to handle potential Integer types safely
            String key = (r.isNullAt(0) ? "" : r.get(0).toString().trim()) + "|" + // REGIME
                         (r.isNullAt(1) ? "" : r.get(1).toString().trim()) + "|" + // RISK_CLASS
                         (r.isNullAt(2) ? "" : r.get(2).toString().trim()) + "|" + // FAMILY
                         (r.isNullAt(3) ? "" : r.get(3).toString().trim()) + "|" + // BUCKET
                         (r.isNullAt(4) ? "" : r.get(4).toString().trim()) + "|" + // NAME_1
                         (r.isNullAt(5) ? "" : r.get(5).toString().trim());        // NAME_2

            // Store correlation values (Medium, Low, High)
            correlationMap.put(key, new double[]{
                r.isNullAt(6) ? 0.0 : ((Number)r.get(6)).doubleValue(),
                r.isNullAt(7) ? 0.0 : ((Number)r.get(7)).doubleValue(),
                r.isNullAt(8) ? 0.0 : ((Number)r.get(8)).doubleValue()
            });
        }

        final Broadcast<Map<String, double[]>> corrBroadcast = sparkSession.sparkContext().broadcast(
            correlationMap, scala.reflect.ClassTag$.MODULE$.apply(Map.class)
        );

        // =====================================================================
        // 2. Capture Field Indices (Driver Side)
        // =====================================================================
        // Indices are captured once to avoid lookups inside the loop
        final int indexRegime = inputDs.schema().fieldIndex("REGIME");
        final int indexRiskClass = inputDs.schema().fieldIndex("RISK_CLASS");
        final int indexScenarioFamily = inputDs.schema().fieldIndex("RISK_FACTOR_SCENARIO_FAMILY");
        final int indexRiskBucket = inputDs.schema().fieldIndex("RISK_BUCKET");
        final int indexRiskFactorName = inputDs.schema().fieldIndex("RISK_FACTOR_NAME");
        final int indexWeightedSensitivity = inputDs.schema().fieldIndex("WEIGHTED_SENSITIVITY");
        final int indexAggregationLevel = inputDs.schema().fieldIndex("AGGREGATION_LEVEL");
        final int indexAggregationLevelValue = inputDs.schema().fieldIndex("AGGREGATION_LEVEL_VALUE");
        final int indexReportingEntityId = inputDs.schema().fieldIndex("REPORTING_ENTITY_COPER_ID");
        final int indexExcludedRiskBucket = inputDs.schema().fieldIndex("EXCLUDED_RISK_BUCKET");

        // =====================================================================
        // 3. RDD Processing Pipeline
        // =====================================================================
        
        // [Step 1] Map inputs to Key-Value Pairs and Group by ALL Bucket Keys
        // Updated logic: Included AGGREGATION_LEVEL, VALUE, and ENTITY_ID in the grouping key
        // This splits the dataset into granular sub-datasets for independent self-joins
        JavaPairRDD<String, Iterable<Row>> bucketedRdd = inputDs.toJavaRDD().mapToPair(r -> {
            
            // Inline null-safe string conversion (replaces getSafeString helper)
            String regime = r.isNullAt(indexRegime) ? "" : r.get(indexRegime).toString().trim();
            String riskClass = r.isNullAt(indexRiskClass) ? "" : r.get(indexRiskClass).toString().trim();
            String family = r.isNullAt(indexScenarioFamily) ? "" : r.get(indexScenarioFamily).toString().trim();
            String bucket = r.isNullAt(indexRiskBucket) ? "" : r.get(indexRiskBucket).toString().trim();
            String entityId = r.isNullAt(indexReportingEntityId) ? "" : r.get(indexReportingEntityId).toString().trim();
            String aggLevel = r.isNullAt(indexAggregationLevel) ? "" : r.get(indexAggregationLevel).toString().trim();
            String aggLevelVal = r.isNullAt(indexAggregationLevelValue) ? "" : r.get(indexAggregationLevelValue).toString().trim();

            // Compound Key ensuring rows are grouped by all relevant dimensions
            String compositeKey = entityId + "|" + aggLevel + "|" + aggLevelVal + "|" + 
                                  regime + "|" + riskClass + "|" + family + "|" + bucket;
                                  
            return new Tuple2<>(compositeKey, r);
        }).groupByKey();

        // [Step 2] Process each Bucket (Parallel Matrix Multiplication)
        JavaRDD<Row> resultRdd = bucketedRdd.map(tuple -> {
            
            // Materialize the iterable into a List for random access (O(1))
            List<Row> rows = new ArrayList<>();
            tuple._2().forEach(rows::add);
            int n = rows.size();
            if (n == 0) return null;

            // Extract context information from the first row in the bucket
            // We use inline checks to ensure safety
            Row first = rows.get(0);
            String riskClass = first.isNullAt(indexRiskClass) ? "" : first.get(indexRiskClass).toString().trim();
            boolean isEquity = "EQUITY".equalsIgnoreCase(riskClass) || "CSR_NON_SEC".equalsIgnoreCase(riskClass);

            // Pre-process data arrays to avoid repeated Row lookups inside the O(N^2) loop
            String[] names = new String[n];
            String[] prefixes = new String[n];
            String[] suffixes = new String[n];
            double[] weights = new double[n];

            for (int i = 0; i < n; i++) {
                Row r = rows.get(i);
                weights[i] = r.isNullAt(indexWeightedSensitivity) ? 0.0 : ((Number)r.get(indexWeightedSensitivity)).doubleValue();
                
                String rawName = r.isNullAt(indexRiskFactorName) ? "" : r.get(indexRiskFactorName).toString().trim();
                names[i] = rawName;

                // Handle Equity Logic: Parse Name into Prefix and Suffix (Issuer)
                if (isEquity) {
                    int lastUnderscore = rawName.lastIndexOf('_');
                    if (lastUnderscore > 0) {
                        prefixes[i] = rawName.substring(0, lastUnderscore).trim();
                        suffixes[i] = rawName.substring(lastUnderscore + 1).trim();
                    } else {
                        prefixes[i] = rawName;
                        suffixes[i] = rawName;
                    }
                }
            }

            // Initialize accumulators
            DoubleAdder sumMed = new DoubleAdder();
            DoubleAdder sumLow = new DoubleAdder();
            DoubleAdder sumHigh = new DoubleAdder();
            Map<String, double[]> lookupData = corrBroadcast.value();

            // Retrieve Key Components for Correlation Lookup (These should match Driver-side map logic)
            // Note: Use the first row's values as they are common for the bucket
            String lookupRegime = first.isNullAt(indexRegime) ? "" : first.get(indexRegime).toString().trim();
            String lookupClass = first.isNullAt(indexRiskClass) ? "" : first.get(indexRiskClass).toString().trim();
            String lookupFamily = first.isNullAt(indexScenarioFamily) ? "" : first.get(indexScenarioFamily).toString().trim();
            String lookupBucket = first.isNullAt(indexRiskBucket) ? "" : first.get(indexRiskBucket).toString().trim();

            // Perform N x N Matrix Multiplication (Parallelized via IntStream)
            IntStream.range(0, n).parallel().forEach(i -> {
                for (int j = 0; j < n; j++) {
                    String lookupKey;
                    
                    if (isEquity) {
                        // For Equity, keys are based on Prefixes
                        // Note: Suffix "diffIssuer" was removed to match the Correlation Map change
                        lookupKey = lookupRegime + "|" + lookupClass + "|" + lookupFamily + "|" + lookupBucket + "|" + 
                                    prefixes[i] + "|" + prefixes[j];
                    } else {
                        // For Non-Equity, keys are based on Full Names
                        lookupKey = lookupRegime + "|" + lookupClass + "|" + lookupFamily + "|" + lookupBucket + "|" + 
                                    names[i] + "|" + names[j];
                    }

                    double[] corrs = lookupData.get(lookupKey);
                    if (corrs != null) {
                        double term = weights[i] * weights[j];
                        sumMed.add(term * corrs[0]);
                        sumLow.add(term * corrs[1]);
                        sumHigh.add(term * corrs[2]);
                    }
                }
            });

            // Construct Final Result Row
            return RowFactory.create(
                first.get(indexAggregationLevel), 
                first.get(indexAggregationLevelValue), 
                first.get(indexRegime), 
                first.get(indexReportingEntityId), 
                first.isNullAt(indexRiskBucket) ? "" : first.get(indexRiskBucket).toString().trim(), // Ensure String output
                first.get(indexRiskClass), 
                first.get(indexScenarioFamily), 
                first.get(indexRiskFactorName),
                sumMed.sum(), 
                sumLow.sum(), 
                sumHigh.sum(), 
                first.get(indexExcludedRiskBucket)
            );
        }).filter(Objects::nonNull);

        // =====================================================================
        // 4. Define Output Schema
        // =====================================================================
        StructType outSchema = DataTypes.createStructType(new StructField[]{
            DataTypes.createStructField("AGGREGATION_LEVEL", DataTypes.StringType, true),
            DataTypes.createStructField("AGGREGATION_LEVEL_VALUE", DataTypes.StringType, true),
            DataTypes.createStructField("REGIME", DataTypes.StringType, true),
            // Ensure data type matches source (using IntegerType here, adjust if source is Long)
            DataTypes.createStructField("REPORTING_ENTITY_COPER_ID", DataTypes.IntegerType, true), 
            DataTypes.createStructField("RISK_BUCKET", DataTypes.StringType, true),
            DataTypes.createStructField("RISK_CLASS", DataTypes.StringType, true),
            DataTypes.createStructField("RISK_FACTOR_SCENARIO_FAMILY", DataTypes.StringType, true),
            DataTypes.createStructField("RISK_FACTOR_NAME", DataTypes.StringType, true),
            DataTypes.createStructField("TERM_OFF_DIAGONAL_MEDIUM", DataTypes.DoubleType, false),
            DataTypes.createStructField("TERM_OFF_DIAGONAL_LOW", DataTypes.DoubleType, false),
            DataTypes.createStructField("TERM_OFF_DIAGONAL_HIGH", DataTypes.DoubleType, false),
            DataTypes.createStructField("EXCLUDED_RISK_BUCKET", DataTypes.StringType, true)
        });

        return sparkSession.createDataFrame(resultRdd, outSchema);
    }
}
























0------------------

import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.broadcast.Broadcast;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;
import scala.Tuple2;

import java.util.*;
import java.util.concurrent.atomic.DoubleAdder;
import java.util.stream.IntStream;

public class SbmStructuredProcessor {

    public Dataset<Row> process(SparkSession spark, Map<String, Dataset<Row>> dsMap) {
        // [Safety Check]
        Dataset<Row> inputDs = dsMap.get("SBM_DELTA_VEGA_SENSITIVITIES");
        Dataset<Row> corrDs = dsMap.get("DELTA_VEGA_INTRA_BUCKET_RESULT_CORRELATION");
        if (inputDs == null || corrDs == null) throw new RuntimeException("Missing required datasets");

        // 1. Broadcast Preparation (Same as your logic, ensuring types are safe)
        List<Row> corrRows = corrDs.select(
            "REGIME", "RISK_CLASS", "RISK_FACTOR_SCENARIO_FAMILY", "RISK_BUCKET",
            "RISK_FACTOR_NAME_1", "RISK_FACTOR_NAME_2", "DIFF_ISSUER",
            "CORRELATION_MEDIUM", "CORRELATION_LOW", "CORRELATION_HIGH"
        ).collectAsList();

        Map<String, double[]> correlationMap = new HashMap<>(corrRows.size());
        for (Row r : corrRows) {
            // Null-safe String concatenation
            String key = (r.isNullAt(0) ? "" : r.getString(0)) + "|" +
                         (r.isNullAt(1) ? "" : r.getString(1)) + "|" +
                         (r.isNullAt(2) ? "" : r.getString(2)) + "|" +
                         (r.isNullAt(3) ? "" : r.getString(3)) + "|" +
                         (r.isNullAt(4) ? "" : r.getString(4).trim()) + "|" +
                         (r.isNullAt(5) ? "" : r.getString(5).trim()) + "|" +
                         (r.isNullAt(6) ? "NULL" : r.getString(6).trim()); // Default to NULL if missing

            correlationMap.put(key, new double[]{
                r.isNullAt(7) ? 0.0 : ((Number)r.get(7)).doubleValue(), // Safer cast
                r.isNullAt(8) ? 0.0 : ((Number)r.get(8)).doubleValue(),
                r.isNullAt(9) ? 0.0 : ((Number)r.get(9)).doubleValue()
            });
        }

        final Broadcast<Map<String, double[]>> corrBroadcast = spark.sparkContext().broadcast(
            correlationMap, scala.reflect.ClassTag$.MODULE$.apply(Map.class)
        );

        // 2. Capture Schema Indices (Done once on Driver)
        final int iReg = inputDs.schema().fieldIndex("REGIME");
        final int iCls = inputDs.schema().fieldIndex("RISK_CLASS");
        final int iFam = inputDs.schema().fieldIndex("RISK_FACTOR_SCENARIO_FAMILY");
        final int iBkt = inputDs.schema().fieldIndex("RISK_BUCKET");
        final int iName = inputDs.schema().fieldIndex("RISK_FACTOR_NAME");
        final int iSens = inputDs.schema().fieldIndex("WEIGHTED_SENSITIVITY");
        final int iAggLvl = inputDs.schema().fieldIndex("AGGREGATION_LEVEL");
        final int iAggVal = inputDs.schema().fieldIndex("AGGREGATION_LEVEL_VALUE");
        final int iRepId = inputDs.schema().fieldIndex("REPORTING_ENTITY_COPER_ID");
        final int iExcBkt = inputDs.schema().fieldIndex("EXCLUDED_RISK_BUCKET");

        // 3. RDD Processing
        JavaPairRDD<String, Iterable<Row>> bucketedRdd = inputDs.toJavaRDD().mapToPair(r -> {
            String k = (r.isNullAt(iReg) ? "" : r.getString(iReg)) + "|" +
                       (r.isNullAt(iCls) ? "" : r.getString(iCls)) + "|" +
                       (r.isNullAt(iFam) ? "" : r.getString(iFam)) + "|" +
                       (r.isNullAt(iBkt) ? "" : r.getString(iBkt)); // Use get(iBkt).toString() if bucket is int
            return new Tuple2<>(k, r);
        }).groupByKey();

        // [FIX] Use map on JavaPairRDD correctly (Tuple2 input)
        JavaRDD<Row> resultRdd = bucketedRdd.map(tuple -> {
            // [FIX] Iterate properly
            List<Row> rows = new ArrayList<>();
            tuple._2().forEach(rows::add);
            int n = rows.size();
            if (n == 0) return null;

            Row first = rows.get(0);
            String riskClass = first.isNullAt(iCls) ? "" : first.getString(iCls);
            boolean isEquity = "EQUITY".equalsIgnoreCase(riskClass) || "CSR_NON_SEC".equalsIgnoreCase(riskClass);

            // Pre-process Arrays
            String[] names = new String[n];
            String[] prefixes = new String[n]; // For Equity name matching
            String[] suffixes = new String[n]; // For Equity issuer matching
            double[] weights = new double[n];

            for (int i = 0; i < n; i++) {
                Row r = rows.get(i);
                weights[i] = r.isNullAt(iSens) ? 0.0 : ((Number)r.get(iSens)).doubleValue();
                String rawName = r.isNullAt(iName) ? "" : r.getString(iName);
                names[i] = rawName;

                if (isEquity) {
                    // Logic: Parse Name (e.g., "EQ_Apple_USD")
                    int lastUnderscore = rawName.lastIndexOf('_');
                    // Prefix: Everything before last underscore
                    // Suffix: Everything after (Issuer/Currency part)
                    if (lastUnderscore > 0) {
                        prefixes[i] = rawName.substring(0, lastUnderscore).trim();
                        suffixes[i] = rawName.substring(lastUnderscore + 1).trim();
                    } else {
                        prefixes[i] = rawName;
                        suffixes[i] = rawName;
                    }
                }
            }

            DoubleAdder sumMed = new DoubleAdder();
            DoubleAdder sumLow = new DoubleAdder();
            DoubleAdder sumHigh = new DoubleAdder();
            Map<String, double[]> lookupData = corrBroadcast.value();

            // Parallel computation for O(N^2)
            IntStream.range(0, n).parallel().forEach(i -> {
                for (int j = 0; j < n; j++) {
                    String lookupKey;
                    if (isEquity) {
                        // [FIX] If suffixes are different -> Diff Issuer = "Y"
                        String diffIssuer = suffixes[i].equals(suffixes[j]) ? "N" : "Y";
                        lookupKey = first.getString(iReg) + "|" + first.getString(iCls) + "|" + first.getString(iFam) + "|" + 
                                    first.getString(iBkt) + "|" + prefixes[i] + "|" + prefixes[j] + "|" + diffIssuer;
                    } else {
                        lookupKey = first.getString(iReg) + "|" + first.getString(iCls) + "|" + first.getString(iFam) + "|" + 
                                    first.getString(iBkt) + "|" + names[i] + "|" + names[j] + "|NULL";
                    }

                    double[] corrs = lookupData.get(lookupKey);
                    if (corrs != null) {
                        double term = weights[i] * weights[j];
                        sumMed.add(term * corrs[0]);
                        sumLow.add(term * corrs[1]);
                        sumHigh.add(term * corrs[2]);
                    }
                }
            });

            return RowFactory.create(
                first.get(iAggLvl), first.get(iAggVal), first.get(iReg), first.get(iRepId),
                first.get(iBkt), first.get(iCls), first.get(iFam), first.get(iName),
                sumMed.sum(), sumLow.sum(), sumHigh.sum(), first.get(iExcBkt)
            );
        }).filter(Objects::nonNull);

        // 4. Output Schema (Ensure types match RowFactory)
        StructType outSchema = DataTypes.createStructType(new StructField[]{
            DataTypes.createStructField("AGGREGATION_LEVEL", DataTypes.StringType, true),
            DataTypes.createStructField("AGGREGATION_LEVEL_VALUE", DataTypes.StringType, true),
            DataTypes.createStructField("REGIME", DataTypes.StringType, true),
            DataTypes.createStructField("REPORTING_ENTITY_COPER_ID", DataTypes.IntegerType, true), // Check source type!
            DataTypes.createStructField("RISK_BUCKET", DataTypes.StringType, true),
            DataTypes.createStructField("RISK_CLASS", DataTypes.StringType, true),
            DataTypes.createStructField("RISK_FACTOR_SCENARIO_FAMILY", DataTypes.StringType, true),
            DataTypes.createStructField("RISK_FACTOR_NAME", DataTypes.StringType, true),
            DataTypes.createStructField("TERM_OFF_DIAGONAL_MEDIUM", DataTypes.DoubleType, false),
            DataTypes.createStructField("TERM_OFF_DIAGONAL_LOW", DataTypes.DoubleType, false),
            DataTypes.createStructField("TERM_OFF_DIAGONAL_HIGH", DataTypes.DoubleType, false),
            DataTypes.createStructField("EXCLUDED_RISK_BUCKET", DataTypes.StringType, true)
        });

        return spark.createDataFrame(resultRdd, outSchema);
    }
}
