// Get the input dataset
Dataset<Row> inputDs = dsMap.get("SBM_DELTA_VEGA_SENSITIVITIES");
if (inputDs == null) throw new RuntimeException("Input missing: SBM_DELTA_VEGA_SENSITIVITIES");

// =====================================================================
// 1. Local Class for Data Transport (Serializable for Cluster Execution)
// =====================================================================
class Factor implements Serializable {
    final String regime, riskClass, riskBucket, family;
    final String tenor, factorType, issuerId;

    Factor(Row r, int[] idx) {
        this.regime = sStr(r.get(idx[0]));
        this.riskClass = sStr(r.get(idx[1]));
        this.riskBucket = sStr(r.get(idx[2]));
        this.family = sStr(r.get(idx[3]));
        this.tenor = sStr(r.get(idx[4]));
        this.factorType = sStr(r.get(idx[5]));
        this.issuerId = sStr(r.get(idx[6]));
    }

    // Key for Grouping: REGIME, RISK_CLASS, RISK_BUCKET, FAMILY
    String getJoinKey() {
        return regime + "|" + riskClass + "|" + riskBucket + "|" + family;
    }

    private String sStr(Object v) { return v == null ? "" : String.valueOf(v); }

    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (!(o instanceof Factor)) return false;
        Factor f = (Factor) o;
        return Objects.equals(regime, f.regime) && Objects.equals(riskClass, f.riskClass) &&
                Objects.equals(riskBucket, f.riskBucket) && Objects.equals(family, f.family) &&
                Objects.equals(tenor, f.tenor) && Objects.equals(factorType, f.factorType) &&
                Objects.equals(issuerId, f.issuerId);
    }

    @Override
    public int hashCode() {
        return Objects.hash(regime, riskClass, riskBucket, family, tenor, factorType, issuerId);
    }
}

// =====================================================================
// 2. Schema Definition (Encapsulated inside process)
// =====================================================================
StructType schema = DataTypes.createStructType(new StructField[]{
        DataTypes.createStructField("REGIME", DataTypes.StringType, false),
        DataTypes.createStructField("RISK_CLASS", DataTypes.StringType, false),
        DataTypes.createStructField("RISK_BUCKET", DataTypes.StringType, false),
        DataTypes.createStructField("RISK_FACTOR_SCENARIO_FAMILY", DataTypes.StringType, false),
        DataTypes.createStructField("RISK_FACTOR_TENOR", DataTypes.StringType, false),
        DataTypes.createStructField("RISK_FACTOR_TYPE", DataTypes.StringType, false),
        DataTypes.createStructField("ISSUER_ID", DataTypes.StringType, false),
        // T1 Aliased Columns (Paired Factors)
        DataTypes.createStructField("RISK_BUCKET_1", DataTypes.StringType, false),
        DataTypes.createStructField("RISK_FACTOR_TENOR_1", DataTypes.StringType, false),
        DataTypes.createStructField("RISK_FACTOR_TYPE_1", DataTypes.StringType, false),
        DataTypes.createStructField("ISSUER_ID_1", DataTypes.StringType, false)
});

// =====================================================================
// 3. Distributed Execution Logic
// =====================================================================

// Cache field indices to avoid repeated schema lookups during mapping
final int[] idx = {
        inputDs.schema().fieldIndex("REGIME"),
        inputDs.schema().fieldIndex("RISK_CLASS"),
        inputDs.schema().fieldIndex("RISK_BUCKET"),
        inputDs.schema().fieldIndex("RISK_FACTOR_SCENARIO_FAMILY"),
        inputDs.schema().fieldIndex("RISK_FACTOR_TENOR"),
        inputDs.schema().fieldIndex("RISK_FACTOR_TYPE"),
        inputDs.schema().fieldIndex("ISSUER_ID")
};

// Step A: Map to Factor objects and apply Global Distinct (SELECT DISTINCT)
JavaPairRDD<String, Factor> groupedRdd = inputDs.toJavaRDD()
        .map(r -> new Factor(r, idx))
        .distinct()
        .mapToPair(f -> new Tuple2<>(f.getJoinKey(), f));

// Step B: Group by Key (Shuffle) to bring factors of the same bucket together
JavaPairRDD<String, Iterable<Factor>> bucketGroups = groupedRdd.groupByKey();

// Step C: Cartesian Product within each bucket (Parallel Join)
JavaPairRDD<Factor, Factor> pairedRdd = bucketGroups.flatMapToPair(tuple -> {
    List<Tuple2<Factor, Factor>> pairs = new ArrayList<>();
    List<Factor> group = new ArrayList<>();
    tuple._2().forEach(group::add);

    // Local N^2 loop on Executor
    for (Factor t : group) {
        for (Factor t1 : group) {
            pairs.add(new Tuple2<>(t, t1));
        }
    }
    return pairs.iterator();
});

// Step D: Map paired objects to Row objects for DataFrame conversion
return spark.createDataFrame(
        pairedRdd.map(p -> RowFactory.create(
                p._1().regime, p._1().riskClass, p._1().riskBucket, p._1().family,
                p._1().tenor, p._1().factorType, p._1().issuerId,
                p._2().riskBucket, p._2().tenor, p._2().factorType, p._2().issuerId
        )),
        schema
);
