import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.broadcast.Broadcast;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;
import scala.Tuple2;

import java.util.*;
import java.util.concurrent.atomic.DoubleAdder;
import java.util.stream.IntStream;

public class SbmStructuredProcessor {

    public Dataset<Row> process(SparkSession spark, Map<String, Dataset<Row>> dsMap) {
        // [Safety Check]
        Dataset<Row> inputDs = dsMap.get("SBM_DELTA_VEGA_SENSITIVITIES");
        Dataset<Row> corrDs = dsMap.get("DELTA_VEGA_INTRA_BUCKET_RESULT_CORRELATION");
        if (inputDs == null || corrDs == null) throw new RuntimeException("Missing required datasets");

        // 1. Broadcast Preparation (Same as your logic, ensuring types are safe)
        List<Row> corrRows = corrDs.select(
            "REGIME", "RISK_CLASS", "RISK_FACTOR_SCENARIO_FAMILY", "RISK_BUCKET",
            "RISK_FACTOR_NAME_1", "RISK_FACTOR_NAME_2", "DIFF_ISSUER",
            "CORRELATION_MEDIUM", "CORRELATION_LOW", "CORRELATION_HIGH"
        ).collectAsList();

        Map<String, double[]> correlationMap = new HashMap<>(corrRows.size());
        for (Row r : corrRows) {
            // Null-safe String concatenation
            String key = (r.isNullAt(0) ? "" : r.getString(0)) + "|" +
                         (r.isNullAt(1) ? "" : r.getString(1)) + "|" +
                         (r.isNullAt(2) ? "" : r.getString(2)) + "|" +
                         (r.isNullAt(3) ? "" : r.getString(3)) + "|" +
                         (r.isNullAt(4) ? "" : r.getString(4).trim()) + "|" +
                         (r.isNullAt(5) ? "" : r.getString(5).trim()) + "|" +
                         (r.isNullAt(6) ? "NULL" : r.getString(6).trim()); // Default to NULL if missing

            correlationMap.put(key, new double[]{
                r.isNullAt(7) ? 0.0 : ((Number)r.get(7)).doubleValue(), // Safer cast
                r.isNullAt(8) ? 0.0 : ((Number)r.get(8)).doubleValue(),
                r.isNullAt(9) ? 0.0 : ((Number)r.get(9)).doubleValue()
            });
        }

        final Broadcast<Map<String, double[]>> corrBroadcast = spark.sparkContext().broadcast(
            correlationMap, scala.reflect.ClassTag$.MODULE$.apply(Map.class)
        );

        // 2. Capture Schema Indices (Done once on Driver)
        final int iReg = inputDs.schema().fieldIndex("REGIME");
        final int iCls = inputDs.schema().fieldIndex("RISK_CLASS");
        final int iFam = inputDs.schema().fieldIndex("RISK_FACTOR_SCENARIO_FAMILY");
        final int iBkt = inputDs.schema().fieldIndex("RISK_BUCKET");
        final int iName = inputDs.schema().fieldIndex("RISK_FACTOR_NAME");
        final int iSens = inputDs.schema().fieldIndex("WEIGHTED_SENSITIVITY");
        final int iAggLvl = inputDs.schema().fieldIndex("AGGREGATION_LEVEL");
        final int iAggVal = inputDs.schema().fieldIndex("AGGREGATION_LEVEL_VALUE");
        final int iRepId = inputDs.schema().fieldIndex("REPORTING_ENTITY_COPER_ID");
        final int iExcBkt = inputDs.schema().fieldIndex("EXCLUDED_RISK_BUCKET");

        // 3. RDD Processing
        JavaPairRDD<String, Iterable<Row>> bucketedRdd = inputDs.toJavaRDD().mapToPair(r -> {
            String k = (r.isNullAt(iReg) ? "" : r.getString(iReg)) + "|" +
                       (r.isNullAt(iCls) ? "" : r.getString(iCls)) + "|" +
                       (r.isNullAt(iFam) ? "" : r.getString(iFam)) + "|" +
                       (r.isNullAt(iBkt) ? "" : r.getString(iBkt)); // Use get(iBkt).toString() if bucket is int
            return new Tuple2<>(k, r);
        }).groupByKey();

        // [FIX] Use map on JavaPairRDD correctly (Tuple2 input)
        JavaRDD<Row> resultRdd = bucketedRdd.map(tuple -> {
            // [FIX] Iterate properly
            List<Row> rows = new ArrayList<>();
            tuple._2().forEach(rows::add);
            int n = rows.size();
            if (n == 0) return null;

            Row first = rows.get(0);
            String riskClass = first.isNullAt(iCls) ? "" : first.getString(iCls);
            boolean isEquity = "EQUITY".equalsIgnoreCase(riskClass) || "CSR_NON_SEC".equalsIgnoreCase(riskClass);

            // Pre-process Arrays
            String[] names = new String[n];
            String[] prefixes = new String[n]; // For Equity name matching
            String[] suffixes = new String[n]; // For Equity issuer matching
            double[] weights = new double[n];

            for (int i = 0; i < n; i++) {
                Row r = rows.get(i);
                weights[i] = r.isNullAt(iSens) ? 0.0 : ((Number)r.get(iSens)).doubleValue();
                String rawName = r.isNullAt(iName) ? "" : r.getString(iName);
                names[i] = rawName;

                if (isEquity) {
                    // Logic: Parse Name (e.g., "EQ_Apple_USD")
                    int lastUnderscore = rawName.lastIndexOf('_');
                    // Prefix: Everything before last underscore
                    // Suffix: Everything after (Issuer/Currency part)
                    if (lastUnderscore > 0) {
                        prefixes[i] = rawName.substring(0, lastUnderscore).trim();
                        suffixes[i] = rawName.substring(lastUnderscore + 1).trim();
                    } else {
                        prefixes[i] = rawName;
                        suffixes[i] = rawName;
                    }
                }
            }

            DoubleAdder sumMed = new DoubleAdder();
            DoubleAdder sumLow = new DoubleAdder();
            DoubleAdder sumHigh = new DoubleAdder();
            Map<String, double[]> lookupData = corrBroadcast.value();

            // Parallel computation for O(N^2)
            IntStream.range(0, n).parallel().forEach(i -> {
                for (int j = 0; j < n; j++) {
                    String lookupKey;
                    if (isEquity) {
                        // [FIX] If suffixes are different -> Diff Issuer = "Y"
                        String diffIssuer = suffixes[i].equals(suffixes[j]) ? "N" : "Y";
                        lookupKey = first.getString(iReg) + "|" + first.getString(iCls) + "|" + first.getString(iFam) + "|" + 
                                    first.getString(iBkt) + "|" + prefixes[i] + "|" + prefixes[j] + "|" + diffIssuer;
                    } else {
                        lookupKey = first.getString(iReg) + "|" + first.getString(iCls) + "|" + first.getString(iFam) + "|" + 
                                    first.getString(iBkt) + "|" + names[i] + "|" + names[j] + "|NULL";
                    }

                    double[] corrs = lookupData.get(lookupKey);
                    if (corrs != null) {
                        double term = weights[i] * weights[j];
                        sumMed.add(term * corrs[0]);
                        sumLow.add(term * corrs[1]);
                        sumHigh.add(term * corrs[2]);
                    }
                }
            });

            return RowFactory.create(
                first.get(iAggLvl), first.get(iAggVal), first.get(iReg), first.get(iRepId),
                first.get(iBkt), first.get(iCls), first.get(iFam), first.get(iName),
                sumMed.sum(), sumLow.sum(), sumHigh.sum(), first.get(iExcBkt)
            );
        }).filter(Objects::nonNull);

        // 4. Output Schema (Ensure types match RowFactory)
        StructType outSchema = DataTypes.createStructType(new StructField[]{
            DataTypes.createStructField("AGGREGATION_LEVEL", DataTypes.StringType, true),
            DataTypes.createStructField("AGGREGATION_LEVEL_VALUE", DataTypes.StringType, true),
            DataTypes.createStructField("REGIME", DataTypes.StringType, true),
            DataTypes.createStructField("REPORTING_ENTITY_COPER_ID", DataTypes.IntegerType, true), // Check source type!
            DataTypes.createStructField("RISK_BUCKET", DataTypes.StringType, true),
            DataTypes.createStructField("RISK_CLASS", DataTypes.StringType, true),
            DataTypes.createStructField("RISK_FACTOR_SCENARIO_FAMILY", DataTypes.StringType, true),
            DataTypes.createStructField("RISK_FACTOR_NAME", DataTypes.StringType, true),
            DataTypes.createStructField("TERM_OFF_DIAGONAL_MEDIUM", DataTypes.DoubleType, false),
            DataTypes.createStructField("TERM_OFF_DIAGONAL_LOW", DataTypes.DoubleType, false),
            DataTypes.createStructField("TERM_OFF_DIAGONAL_HIGH", DataTypes.DoubleType, false),
            DataTypes.createStructField("EXCLUDED_RISK_BUCKET", DataTypes.StringType, true)
        });

        return spark.createDataFrame(resultRdd, outSchema);
    }
}
