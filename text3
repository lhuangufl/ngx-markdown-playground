import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.broadcast.Broadcast;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;
import scala.Tuple2;

import java.util.*;
import java.util.stream.IntStream;

public class SbmStructuredProcessor {

    public Dataset<Row> process(SparkSession sparkSession, Map<String, Dataset<Row>> dsMap) {
        
        // =====================================================================
        // 1. Safety Check & Input Retrieval
        // =====================================================================
        Dataset<Row> inputDs = dsMap.get("SBM_DELTA_VEGA_SENSITIVITIES");
        Dataset<Row> corrDs = dsMap.get("DELTA_VEGA_INTRA_BUCKET_RESULT_CORRELATION");

        if (inputDs == null || corrDs == null) throw new RuntimeException("Missing required datasets");

        // =====================================================================
        // 2. Broadcast Preparation (Driver Side)
        // =====================================================================
        List<Row> corrRows = corrDs.select(
            "REGIME", "RISK_CLASS", "RISK_FACTOR_SCENARIO_FAMILY", "RISK_BUCKET",
            "RISK_FACTOR_NAME_1", "RISK_FACTOR_NAME_2",
            "CORRELATION_MEDIUM", "CORRELATION_LOW", "CORRELATION_HIGH"
        ).collectAsList();

        Map<String, double[]> correlationMap = new HashMap<>(corrRows.size());
        for (Row r : corrRows) {
            // Inline null-safe logic to avoid helper methods
            String key = (r.isNullAt(0) ? "" : r.get(0).toString().trim()) + "|" +
                         (r.isNullAt(1) ? "" : r.get(1).toString().trim()) + "|" +
                         (r.isNullAt(2) ? "" : r.get(2).toString().trim()) + "|" +
                         (r.isNullAt(3) ? "" : r.get(3).toString().trim()) + "|" +
                         (r.isNullAt(4) ? "" : r.get(4).toString().trim()) + "|" +
                         (r.isNullAt(5) ? "" : r.get(5).toString().trim());

            correlationMap.put(key, new double[]{
                r.isNullAt(6) ? 0.0 : ((Number)r.get(6)).doubleValue(),
                r.isNullAt(7) ? 0.0 : ((Number)r.get(7)).doubleValue(),
                r.isNullAt(8) ? 0.0 : ((Number)r.get(8)).doubleValue()
            });
        }

        final Broadcast<Map<String, double[]>> corrBroadcast = sparkSession.sparkContext().broadcast(
            correlationMap, scala.reflect.ClassTag$.MODULE$.apply(Map.class)
        );

        // =====================================================================
        // 3. Configuration Packing (The "Anti-Closure" Strategy)
        // =====================================================================
        // We pack all indices into a serializable int[] array.
        // This array will be passed to inner classes via constructor.
        // Array Layout:
        // [0]=Regime, [1]=Class, [2]=Family, [3]=Bucket, [4]=Name, [5]=Sens, 
        // [6]=AggLvl, [7]=AggVal, [8]=EntityId
        int[] indexConfig = new int[] {
            inputDs.schema().fieldIndex("REGIME"),
            inputDs.schema().fieldIndex("RISK_CLASS"),
            inputDs.schema().fieldIndex("RISK_FACTOR_SCENARIO_FAMILY"),
            inputDs.schema().fieldIndex("RISK_BUCKET"),
            inputDs.schema().fieldIndex("RISK_FACTOR_NAME"),
            inputDs.schema().fieldIndex("WEIGHTED_SENSITIVITY"),
            inputDs.schema().fieldIndex("AGGREGATION_LEVEL"),
            inputDs.schema().fieldIndex("AGGREGATION_LEVEL_VALUE"),
            inputDs.schema().fieldIndex("REPORTING_ENTITY_COPER_ID"),
            inputDs.schema().fieldIndex("EXCLUDED_RISK_BUCKET") // Index 9
        };

        // =====================================================================
        // 4. Define Logic Classes INSIDE process()
        // =====================================================================

        /**
         * Mapper Class for Grouping.
         * Defined locally to ensure encapsulation, but uses explicit state (idx array)
         * to avoid capturing the outer class reference.
         */
        class GroupingMapper implements PairFunction<Row, String, Row> {
            private final int[] idx;

            // Explicit Constructor: Receives state manually
            GroupingMapper(int[] idx) {
                this.idx = idx;
            }

            @Override
            public Tuple2<String, Row> call(Row r) {
                // Accessing indices via local array 'idx', NOT outer variables
                String k = (r.isNullAt(idx[8]) ? "" : r.get(idx[8]).toString().trim()) + "|" + // EntityID
                           (r.isNullAt(idx[6]) ? "" : r.get(idx[6]).toString().trim()) + "|" + // AggLvl
                           (r.isNullAt(idx[7]) ? "" : r.get(idx[7]).toString().trim()) + "|" + // AggVal
                           (r.isNullAt(idx[0]) ? "" : r.get(idx[0]).toString().trim()) + "|" + // Regime
                           (r.isNullAt(idx[1]) ? "" : r.get(idx[1]).toString().trim()) + "|" + // Class
                           (r.isNullAt(idx[2]) ? "" : r.get(idx[2]).toString().trim()) + "|" + // Family
                           (r.isNullAt(idx[3]) ? "" : r.get(idx[3]).toString().trim());        // Bucket
                return new Tuple2<>(k, r);
            }
        }

        /**
         * Calculator Class for Matrix Multiplication.
         * Defined locally. Receives Broadcast and Indices via constructor.
         */
        class MatrixCalculator implements FlatMapFunction<Tuple2<String, Iterable<Row>>, Row> {
            private final int[] idx;
            private final Broadcast<Map<String, double[]>> broadcast;

            // Explicit Constructor
            MatrixCalculator(int[] idx, Broadcast<Map<String, double[]>> broadcast) {
                this.idx = idx;
                this.broadcast = broadcast;
            }

            @Override
            public Iterator<Row> call(Tuple2<String, Iterable<Row>> tuple) {
                // Materialize rows
                List<Row> rows = new ArrayList<>();
                for (Row r : tuple._2()) {
                    rows.add(r);
                }
                int n = rows.size();
                if (n == 0) return Collections.emptyIterator();

                // Context info
                Row first = rows.get(0);
                String riskClass = first.isNullAt(idx[1]) ? "" : first.get(idx[1]).toString().trim();
                boolean isEquity = "EQUITY".equalsIgnoreCase(riskClass) || "CSR_NON_SEC".equalsIgnoreCase(riskClass);

                // Pre-process Arrays
                String[] names = new String[n];
                String[] prefixes = new String[n];
                String[] suffixes = new String[n];
                double[] weights = new double[n];

                for (int i = 0; i < n; i++) {
                    Row r = rows.get(i);
                    // Handle Sensitivity (idx[5])
                    weights[i] = r.isNullAt(idx[5]) ? 0.0 : ((Number)r.get(idx[5])).doubleValue();
                    
                    // Handle Name (idx[4])
                    String rawName = r.isNullAt(idx[4]) ? "" : r.get(idx[4]).toString().trim();
                    names[i] = rawName;

                    if (isEquity) {
                        int lastUnderscore = rawName.lastIndexOf('_');
                        if (lastUnderscore > 0) {
                            prefixes[i] = rawName.substring(0, lastUnderscore).trim();
                            suffixes[i] = rawName.substring(lastUnderscore + 1).trim();
                        } else {
                            prefixes[i] = rawName;
                            suffixes[i] = rawName;
                        }
                    }
                }

                Map<String, double[]> lookupData = broadcast.value();
                
                // Pre-calculate common key prefix
                String regime = first.isNullAt(idx[0]) ? "" : first.get(idx[0]).toString().trim();
                String family = first.isNullAt(idx[2]) ? "" : first.get(idx[2]).toString().trim();
                String bucket = first.isNullAt(idx[3]) ? "" : first.get(idx[3]).toString().trim();
                
                String commonPrefix = regime + "|" + riskClass + "|" + family + "|" + bucket + "|";

                List<Row> results = new ArrayList<>(n);

                // Calculation Loop
                for (int i = 0; i < n; i++) {
                    double sumMed = 0.0;
                    double sumLow = 0.0;
                    double sumHigh = 0.0;

                    for (int j = 0; j < n; j++) {
                        String lookupKey;
                        if (isEquity) {
                            lookupKey = commonPrefix + prefixes[i] + "|" + prefixes[j];
                        } else {
                            lookupKey = commonPrefix + names[i] + "|" + names[j];
                        }

                        double[] corrs = lookupData.get(lookupKey);
                        if (corrs != null) {
                            double term = weights[i] * weights[j];
                            sumMed += term * corrs[0];
                            sumLow += term * corrs[1];
                            sumHigh += term * corrs[2];
                        }
                    }

                    results.add(RowFactory.create(
                        first.get(idx[6]), // AggLevel
                        first.get(idx[7]), // AggVal
                        first.get(idx[0]), // Regime
                        first.get(idx[8]), // EntityID
                        first.isNullAt(idx[3]) ? "" : first.get(idx[3]).toString().trim(), // Bucket
                        first.get(idx[1]), // RiskClass
                        first.get(idx[2]), // Family
                        names[i], 
                        sumMed, 
                        sumLow, 
                        sumHigh, 
                        weights[i] // WEIGHTED_SENSITIVITY_K
                    ));
                }

                return results.iterator();
            }
        }

        // =====================================================================
        // 5. Execution Pipeline
        // =====================================================================
        
        // Pass the 'indexConfig' array to constructor to avoid closure capture
        JavaPairRDD<String, Iterable<Row>> bucketedRdd = inputDs.toJavaRDD()
            .mapToPair(new GroupingMapper(indexConfig))
            .groupByKey();

        // Pass 'indexConfig' and 'corrBroadcast' to constructor
        JavaRDD<Row> resultRdd = bucketedRdd.flatMap(new MatrixCalculator(indexConfig, corrBroadcast));

        // =====================================================================
        // 6. Output Schema
        // =====================================================================
        StructType outSchema = DataTypes.createStructType(new StructField[]{
            DataTypes.createStructField("AGGREGATION_LEVEL", DataTypes.StringType, true),
            DataTypes.createStructField("AGGREGATION_LEVEL_VALUE", DataTypes.StringType, true),
            DataTypes.createStructField("REGIME", DataTypes.StringType, true),
            DataTypes.createStructField("REPORTING_ENTITY_COPER_ID", DataTypes.IntegerType, true), 
            DataTypes.createStructField("RISK_BUCKET", DataTypes.StringType, true),
            DataTypes.createStructField("RISK_CLASS", DataTypes.StringType, true),
            DataTypes.createStructField("RISK_FACTOR_SCENARIO_FAMILY", DataTypes.StringType, true),
            DataTypes.createStructField("RISK_FACTOR_NAME", DataTypes.StringType, true),
            DataTypes.createStructField("TERM_OFF_DIAGONAL_MEDIUM", DataTypes.DoubleType, false),
            DataTypes.createStructField("TERM_OFF_DIAGONAL_LOW", DataTypes.DoubleType, false),
            DataTypes.createStructField("TERM_OFF_DIAGONAL_HIGH", DataTypes.DoubleType, false),
            DataTypes.createStructField("WEIGHTED_SENSITIVITY_K", DataTypes.DoubleType, false)
        });

        return sparkSession.createDataFrame(resultRdd, outSchema);
