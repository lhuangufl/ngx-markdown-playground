String diffIssuer = "Y"; // 默认不同
                        if (issuers[i] != null && issuers[j] != null && issuers[i].equals(issuers[j])) {
                            diffIssuer = "N"; // 相同
                        }
                        
                        // 2. 构造 Key，匹配 Driver 端生成的 "Y|Y" 或 "N|N" 这种格式
                        // 注意：这里假设相关性表中 Name_1 和 Name_2 的前缀逻辑是一致的
                        lookupKey = commonPrefix + diffIssuer + "|" + diffIssuer;



for (Row r : corrRows) {
        String rClass = r.isNullAt(1) ? "" : r.get(1).toString().trim();
        String name1 = r.isNullAt(4) ? "" : r.get(4).toString().trim();
        String name2 = r.isNullAt(5) ? "" : r.get(5).toString().trim();

        // [关键修正]：如果是 Equity/CSR，SQL 逻辑是匹配 Name 的第一个字符 (Y/N)
        boolean isEquityRule = "EQUITY".equalsIgnoreCase(rClass) || "CSR_NON_SEC".equalsIgnoreCase(rClass);
        
        String keyPart1 = name1;
        String keyPart2 = name2;

        if (isEquityRule) {
            // 对应 SQL: TRIM(SUBSTR(CORRELATION.RISK_FACTOR_NAME_1, 1, 1))
            // 我们在 Map Key 中只存 "Y" 或 "N"，方便 O(1) 查找
            if (name1.length() > 0) keyPart1 = name1.substring(0, 1);
            // 注意：SQL 截图显示 Name_1 和 Name_2 都要匹配 Diff Issuer，
            // 通常 Equity 相关性表里这两个值是一样的 (e.g., Y vs Y)，这里我们统一处理
            if (name2.length() > 0) keyPart2 = name2.substring(0, 1);
        }

        // 生成 Map Key
        String key = (r.isNullAt(0) ? "" : r.get(0).toString().trim()) + "|" + // REGIME
                     rClass + "|" +                                            // RISK_CLASS
                     (r.isNullAt(2) ? "" : r.get(2).toString().trim()) + "|" + // FAMILY
                     (r.isNullAt(3) ? "" : r.get(3).toString().trim()) + "|" + // BUCKET
                     keyPart1 + "|" +                                          // Name1 (or Y/N)
                     keyPart2;                                                 // Name2 (or Y/N)

        correlationMap.put(key, new double[]{
            r.isNullAt(6) ? 0.0 : ((Number)r.get(6)).doubleValue(),
            r.isNullAt(7) ? 0.0 : ((Number)r.get(7)).doubleValue(),
            r.isNullAt(8) ? 0.0 : ((Number)r.get(8)).doubleValue()
        });









import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.Map;
import java.util.HashMap;
import java.util.Objects;
import java.util.concurrent.atomic.DoubleAdder;
import java.util.stream.IntStream;
import java.util.stream.Collectors;

public class SbmStructuredProcessor {

    public Dataset<Row> process(SparkSession sparkSession, Map<String, Dataset<Row>> dsMap) {
        
        // =====================================================================
        // 1. Input Retrieval & Safety Check
        // =====================================================================
        Dataset<Row> inputDs = dsMap.get("SBM_DELTA_VEGA_SENSITIVITIES");
        Dataset<Row> corrDs = dsMap.get("DELTA_VEGA_INTRA_BUCKET_RESULT_CORRELATION");

        if (inputDs == null || corrDs == null) throw new RuntimeException("Missing required datasets");

        // =====================================================================
        // 2. Prepare Correlation Lookup (Driver Local Map)
        // =====================================================================
        // Constraint: Cannot use Broadcast class, so we use a standard local Map.
        List<Row> corrRows = corrDs.select(
            "REGIME", "RISK_CLASS", "RISK_FACTOR_SCENARIO_FAMILY", "RISK_BUCKET",
            "RISK_FACTOR_NAME_1", "RISK_FACTOR_NAME_2",
            "CORRELATION_MEDIUM", "CORRELATION_LOW", "CORRELATION_HIGH"
        ).collectAsList();

        Map<String, double[]> correlationMap = new HashMap<>(corrRows.size());
        
        for (Row r : corrRows) {
            // Inline null-safe key generation
            String key = (r.isNullAt(0) ? "" : r.get(0).toString().trim()) + "|" +
                         (r.isNullAt(1) ? "" : r.get(1).toString().trim()) + "|" +
                         (r.isNullAt(2) ? "" : r.get(2).toString().trim()) + "|" +
                         (r.isNullAt(3) ? "" : r.get(3).toString().trim()) + "|" +
                         (r.isNullAt(4) ? "" : r.get(4).toString().trim()) + "|" +
                         (r.isNullAt(5) ? "" : r.get(5).toString().trim());

            correlationMap.put(key, new double[]{
                r.isNullAt(6) ? 0.0 : ((Number)r.get(6)).doubleValue(),
                r.isNullAt(7) ? 0.0 : ((Number)r.get(7)).doubleValue(),
                r.isNullAt(8) ? 0.0 : ((Number)r.get(8)).doubleValue()
            });
        }

        // =====================================================================
        // 3. Collect & Group Data (Manual GroupBy)
        // =====================================================================
        // Constraint: Cannot use RDD or Dataset functional ops (no MapFunction imported).
        // Solution: Collect to Driver and group manually using Java Map.
        List<Row> allInputRows = inputDs.collectAsList();
        
        // Indices setup
        final int idxRegime = inputDs.schema().fieldIndex("REGIME");
        final int idxRiskClass = inputDs.schema().fieldIndex("RISK_CLASS");
        final int idxFamily = inputDs.schema().fieldIndex("RISK_FACTOR_SCENARIO_FAMILY");
        final int idxBucket = inputDs.schema().fieldIndex("RISK_BUCKET");
        final int idxName = inputDs.schema().fieldIndex("RISK_FACTOR_NAME");
        final int idxSens = inputDs.schema().fieldIndex("WEIGHTED_SENSITIVITY");
        final int idxAggLvl = inputDs.schema().fieldIndex("AGGREGATION_LEVEL");
        final int idxAggVal = inputDs.schema().fieldIndex("AGGREGATION_LEVEL_VALUE");
        final int idxEntity = inputDs.schema().fieldIndex("REPORTING_ENTITY_COPER_ID");

        Map<String, List<Row>> bucketGroups = new HashMap<>();

        for (Row r : allInputRows) {
            // Grouping Key: Entity|AggLevel|AggVal|Regime|Class|Family|Bucket
            String k = (r.isNullAt(idxEntity) ? "" : r.get(idxEntity).toString().trim()) + "|" +
                       (r.isNullAt(idxAggLvl) ? "" : r.get(idxAggLvl).toString().trim()) + "|" +
                       (r.isNullAt(idxAggVal) ? "" : r.get(idxAggVal).toString().trim()) + "|" +
                       (r.isNullAt(idxRegime) ? "" : r.get(idxRegime).toString().trim()) + "|" +
                       (r.isNullAt(idxRiskClass) ? "" : r.get(idxRiskClass).toString().trim()) + "|" +
                       (r.isNullAt(idxFamily) ? "" : r.get(idxFamily).toString().trim()) + "|" +
                       (r.isNullAt(idxBucket) ? "" : r.get(idxBucket).toString().trim());

            bucketGroups.computeIfAbsent(k, x -> new ArrayList<>()).add(r);
        }

        // =====================================================================
        // 4. Matrix Calculation (Parallel Stream)
        // =====================================================================
        // Thread-safe list to hold final results
        List<Row> finalResults = Collections.synchronizedList(new ArrayList<>());

        // Iterate over each bucket (Group)
        // We use bucketGroups.entrySet().parallelStream() to utilize multi-core on Driver
        bucketGroups.entrySet().parallelStream().forEach(entry -> {
            List<Row> rows = entry.getValue();
            int n = rows.size();
            if (n == 0) return;

            // Context from first row
            Row first = rows.get(0);
            String riskClass = first.isNullAt(idxRiskClass) ? "" : first.get(idxRiskClass).toString().trim();
            boolean isEquity = "EQUITY".equalsIgnoreCase(riskClass) || "CSR_NON_SEC".equalsIgnoreCase(riskClass);

            // Pre-process Arrays
            String[] names = new String[n];
            String[] prefixes = new String[n];
            double[] weights = new double[n];

            for (int i = 0; i < n; i++) {
                Row r = rows.get(i);
                weights[i] = r.isNullAt(idxSens) ? 0.0 : ((Number)r.get(idxSens)).doubleValue();
                String rawName = r.isNullAt(idxName) ? "" : r.get(idxName).toString().trim();
                names[i] = rawName;

                if (isEquity) {
                    int lastUnderscore = rawName.lastIndexOf('_');
                    if (lastUnderscore > 0) {
                        prefixes[i] = rawName.substring(0, lastUnderscore).trim();
                    } else {
                        prefixes[i] = rawName;
                    }
                }
            }

            // Common lookup prefix
            String commonPrefix = (first.isNullAt(idxRegime) ? "" : first.get(idxRegime).toString().trim()) + "|" +
                                  riskClass + "|" +
                                  (first.isNullAt(idxFamily) ? "" : first.get(idxFamily).toString().trim()) + "|" +
                                  (first.isNullAt(idxBucket) ? "" : first.get(idxBucket).toString().trim()) + "|";

            // N x N Matrix Loop
            IntStream.range(0, n).forEach(i -> {
                double sumMed = 0.0;
                double sumLow = 0.0;
                double sumHigh = 0.0;

                for (int j = 0; j < n; j++) {
                    String lookupKey;
                    if (isEquity) {
                        lookupKey = commonPrefix + prefixes[i] + "|" + prefixes[j];
                    } else {
                        lookupKey = commonPrefix + names[i] + "|" + names[j];
                    }

                    double[] corrs = correlationMap.get(lookupKey);
                    if (corrs != null) {
                        double term = weights[i] * weights[j];
                        sumMed += term * corrs[0];
                        sumLow += term * corrs[1];
                        sumHigh += term * corrs[2];
                    }
                }

                // Add to result
                finalResults.add(RowFactory.create(
                    first.get(idxAggLvl),
                    first.get(idxAggVal),
                    first.get(idxRegime),
                    first.get(idxEntity),
                    first.isNullAt(idxBucket) ? "" : first.get(idxBucket).toString().trim(),
                    first.get(idxRiskClass),
                    first.get(idxFamily),
                    names[i],
                    sumMed,
                    sumLow,
                    sumHigh,
                    weights[i] // WEIGHTED_SENSITIVITY_K
                ));
            });
        });

        // =====================================================================
        // 5. Output Schema & Return
        // =====================================================================
        StructType outSchema = DataTypes.createStructType(new StructField[]{
            DataTypes.createStructField("AGGREGATION_LEVEL", DataTypes.StringType, true),
            DataTypes.createStructField("AGGREGATION_LEVEL_VALUE", DataTypes.StringType, true),
            DataTypes.createStructField("REGIME", DataTypes.StringType, true),
            DataTypes.createStructField("REPORTING_ENTITY_COPER_ID", DataTypes.IntegerType, true), 
            DataTypes.createStructField("RISK_BUCKET", DataTypes.StringType, true),
            DataTypes.createStructField("RISK_CLASS", DataTypes.StringType, true),
            DataTypes.createStructField("RISK_FACTOR_SCENARIO_FAMILY", DataTypes.StringType, true),
            DataTypes.createStructField("RISK_FACTOR_NAME", DataTypes.StringType, true),
            DataTypes.createStructField("TERM_OFF_DIAGONAL_MEDIUM", DataTypes.DoubleType, false),
            DataTypes.createStructField("TERM_OFF_DIAGONAL_LOW", DataTypes.DoubleType, false),
            DataTypes.createStructField("TERM_OFF_DIAGONAL_HIGH", DataTypes.DoubleType, false),
            DataTypes.createStructField("WEIGHTED_SENSITIVITY_K", DataTypes.DoubleType, false)
        });

        // Convert List<Row> back to Dataset<Row>
        return sparkSession.createDataFrame(finalResults, outSchema);
    }
}
